{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "<span style=\"float: right; color: rgb(128, 128, 128); font-size:150%\" >  <strong> Final Project - Milestone 1</span>\n",
    "<span style=\"float: left; color: rgb(128, 128, 128); font-size:150%\"> <strong>  42186 Model-based machine learning (F20)</span>    \n",
    "    <br>  \n",
    "  \n",
    "    \n",
    "    \n",
    "  <span style=\"float: right; color: rgb(128, 128, 128)\" >  <strong> Students:</span>\n",
    "  <span style=\"float: left; color: rgb(128, 128, 128)\"> <strong>  Professors:</span>\n",
    "<div> \n",
    "      <br>  \n",
    "<div>\n",
    "  <span style=\"float: right\" > Guðlaug Erlendsdóttir, s185717</span> \n",
    "  <span style=\"float: left\"> Filipe Rodrigues</span>\n",
    "<div> \n",
    "    <br>\n",
    "  <span style=\"float: right\" > Matthías Karl Karlsson, s182306</span> \n",
    "  <span style=\"float: left\"> Francisco Camara Pereira</span>\n",
    "    <br>\n",
    "<div style=\"text-align: right\"> Steinn Orri Erlendsson, s153716</div>\n",
    "    \n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "### [1. Description of Project](#one)\n",
    "\n",
    "* ####  [1.1. Research Question](#ResearchQuestion)\n",
    "\n",
    "### [2. Data](#two)\n",
    "\n",
    "* #### [2.1 Data Collection and Preprocessing](#two.one)\n",
    "\n",
    "### [3.Natural Language Preprocessing](#three)\n",
    "\n",
    "### [4. Descriptive Stats](#four)\n",
    "\n",
    "### [5. Linear Regression - For comparison](#reg)\n",
    "\n",
    "### [6. Generative Story & PGM](#six)\n",
    "\n",
    "### [7. Inital STAN Model](#eight)\n",
    "\n",
    "### [ Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simpsons - Topic Modeling with Latent Dirichlet Allocation(LDA)\n",
    "\n",
    "\n",
    "In this notebook, topic modeling through Latent Dirichlet Allocation(LDA) is used in order to try uncover the topics that have come up on episodes of The Simpsons through the years. \n",
    "\n",
    "Attributes such as `rating` and `viewers` are extracted from [IMDB](IMDB) for the first 600 episodes of The Simpsons. \n",
    "Then, for each of these episodes, the description is scraped from [The Simpsons Fandom](page) page. \n",
    "There is a lot of data cleaning to be done, as there are a lot of missing values that resulted from the web scraping due to some inconstistency in the pages' setup. \n",
    "Natural Language Processing is deployed in order to represent the textual data better. Each episode's description was put through *TF-IDF analysis* in order to obtain a better representation of their description. \n",
    "\n",
    "By using LDA for topic modeling, the episodes can be seen as a mixture of topics, and therefore a relationhip between what is happening in the episode and its rating could potentially be made. Finally, a simple linear regression is used for comparison of the model's performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Description of Project<a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "For those that have ever watched the television show *The Simpsons*, it is quite known that the quality of episodes have dwindled over the past years. The show has been running for 30 years and it has long gone past its glory days. But what is the reason behind this decline in quality? In this notebook, the topics of each episode of *The Simpsons* is investigated and through *topic modeling* episodes are clustered together in order to see whether the topics of episodes have anything to do with their respective ratings. \n",
    "\n",
    "Ratings for 600 episodes were collected from [IMDB](https://www.imdb.com/) and descriptions for each of those episodes was collected from [The Simpsons Wiki Fanpage](https://simpsons.fandom.com/wiki/List_of_Episodes). This information, along with additional information collected from [Wikipedia](https://www.wikipedia.org) such as viewers per episode and airdate, was used in order to create a data set.\n",
    "\n",
    "The probabilistic topic model *Latent Dirichlet Allocation* was used in order to extract topic proportions and allocations for each episode. These topics were then investigated in order to see if there is any apparent relationship between the topic of episodes over the years and their declining ratings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Research Question<a class=\"anchor\" id=\"ResearchQuestion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using *Topic Modeling* and *Latent Dirichlet Allocation*, we hope to uncover some of the hidden features of the episodes. In general, an episode's rating is related to what is happening in the episode. We want to find if there a relationship between the topic of episodes and their respective ratings, so our research question is;\n",
    "\n",
    "> How do the topic proportions align with the episodes and can they be used to determine an episode's rating?\n",
    "\n",
    "If the topic allocations and proportions of an episode can help determine an episode's rating, not only have be created a better linear regression model, but we have proven a hypothesis which does seem logical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data<a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "In this section, the method of collecting and preprocessing both the *ratings* and *description* for each episode is shown. All of the data is collected through web scraping. The websites that were scraped were [IMDB](https://www.imdb.com), [Wikipedia](https://www.wikipedia.org), and [The Simpsons Fandom Wiki Page](https://simpsons.fandom.com/wiki/List_of_Episodes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width: 95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "import nltk, re, pprint\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import collections\n",
    "import colorsys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import Image\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "import pystan\n",
    "import pystan_utils\n",
    "#make margins smaller \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width: 95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Collection and Preprocessing<a class=\"anchor\" id=\"two.one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Information <a class=\"anchor\" id=\"two.one.one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all episode names, dates and relevant information is possible by scraping Wikipedia. This will serve as a basis for the episode information which will be used throughout the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_1_20_url = 'https://en.wikipedia.org/wiki/List_of_The_Simpsons_episodes_(seasons_1%E2%80%9320)#Episodes'\n",
    "episode_21_31_url = \"https://en.wikipedia.org/wiki/List_of_The_Simpsons_episodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(episode_1_20_url).text\n",
    "soup = BeautifulSoup(res,'html')\n",
    "arr = []\n",
    "next_season = 1\n",
    "for table in soup.find_all('table', class_='wikiepisodetable'):\n",
    "    if(table.previous.previous.previous.text != 'The Simpsons Movie'):\n",
    "        season = table.previous.previous.previous.text.split('(')[1].split(')')[0].split(\" \")[1]\n",
    "        for row in table.find_all('tr')[1::1]:\n",
    "            data = row.find_all(['th','td'])\n",
    "            no = data[0].text\n",
    "            no_in_season = data[1].text\n",
    "            episode_name = data[2].text[1:len(data[2].text)-1] #Remove the \" \" \n",
    "            airdate = data[5].text\n",
    "            viewers = data[7].text.split(\"[\")[0]\n",
    "            arr.append([no,season,no_in_season,episode_name,airdate,viewers])\n",
    "df_1 = pd.DataFrame(arr)\n",
    "df_1.columns = ['no_overall','season','no_in_season','episode_name','airdate','viewers (millions)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasons 1 to 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>December 17, 1989 (1989-12-17)</td>\n",
       "      <td>26.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart the Genius</td>\n",
       "      <td>January 14, 1990 (1990-01-14)</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer's Odyssey</td>\n",
       "      <td>January 21, 1990 (1990-01-21)</td>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  no_overall season no_in_season                       episode_name  \\\n",
       "0          1      1            1  Simpsons Roasting on an Open Fire   \n",
       "1          2      1            2                    Bart the Genius   \n",
       "2          3      1            3                    Homer's Odyssey   \n",
       "\n",
       "                          airdate viewers (millions)  \n",
       "0  December 17, 1989 (1989-12-17)               26.7  \n",
       "1   January 14, 1990 (1990-01-14)               24.5  \n",
       "2   January 21, 1990 (1990-01-21)               27.5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>439</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>Waverly Hills, 9-0-2-1-D'oh</td>\n",
       "      <td>May 3, 2009 (2009-05-03)</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>440</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Four Great Women and a Manicure</td>\n",
       "      <td>May 10, 2009 (2009-05-10)</td>\n",
       "      <td>5.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>441</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>Coming to Homerica</td>\n",
       "      <td>May 17, 2009 (2009-05-17)</td>\n",
       "      <td>5.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_overall season no_in_season                     episode_name  \\\n",
       "438        439     20           19      Waverly Hills, 9-0-2-1-D'oh   \n",
       "439        440     20           20  Four Great Women and a Manicure   \n",
       "440        441     20           21               Coming to Homerica   \n",
       "\n",
       "                       airdate viewers (millions)  \n",
       "438   May 3, 2009 (2009-05-03)               6.75  \n",
       "439  May 10, 2009 (2009-05-10)               5.16  \n",
       "440  May 17, 2009 (2009-05-17)               5.86  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the rest of the seasons are retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(episode_21_31_url).text\n",
    "soup = BeautifulSoup(res,'html')\n",
    "arr = []\n",
    "next_season = 1\n",
    "for table in soup.find_all('table', class_='wikiepisodetable'):\n",
    "    if(table.previous.previous.previous.text != 'The Longest Daycare' and table.previous.previous.previous.text):\n",
    "        season = table.previous.previous.previous.text.split('(')[1].split(')')[0].split(\" \")[1]\n",
    "        for row in table.find_all('tr')[1::1]:\n",
    "            data = row.find_all(['th','td'])\n",
    "            no = data[0].text\n",
    "            no_in_season = data[1].text\n",
    "            episode_name = data[2].text[1:len(data[2].text)-1] #Remove the \" \" \n",
    "            airdate = data[5].text\n",
    "            viewers = data[7].text.split(\"[\")[0]\n",
    "            arr.append([no,season,no_in_season,episode_name,airdate,viewers])\n",
    "        if(season=='30'):\n",
    "            break\n",
    "df_2 = pd.DataFrame(arr)\n",
    "df_2.columns = ['no_overall','season','no_in_season','episode_name','airdate','viewers (millions)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasons 21 to 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>442</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer the Whopper</td>\n",
       "      <td>September 27, 2009 (2009-09-27)</td>\n",
       "      <td>8.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>443</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart Gets a 'Z'</td>\n",
       "      <td>October 4, 2009 (2009-10-04)</td>\n",
       "      <td>9.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>444</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>The Great Wife Hope</td>\n",
       "      <td>October 11, 2009 (2009-10-11)</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  no_overall season no_in_season         episode_name  \\\n",
       "0        442     21            1    Homer the Whopper   \n",
       "1        443     21            2      Bart Gets a 'Z'   \n",
       "2        444     21            3  The Great Wife Hope   \n",
       "\n",
       "                           airdate viewers (millions)  \n",
       "0  September 27, 2009 (2009-09-27)               8.31  \n",
       "1     October 4, 2009 (2009-10-04)               9.32  \n",
       "2    October 11, 2009 (2009-10-11)                7.5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>660</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>D'oh Canada</td>\n",
       "      <td>April 28, 2019 (2019-04-28)</td>\n",
       "      <td>1.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>661</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>Woo-Hoo Dunnit?</td>\n",
       "      <td>May 5, 2019 (2019-05-05)</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>662</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>Crystal Blue-Haired Persuasion</td>\n",
       "      <td>May 12, 2019 (2019-05-12)</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_overall season no_in_season                    episode_name  \\\n",
       "217        660     30           21                     D'oh Canada   \n",
       "218        661     30           22                 Woo-Hoo Dunnit?   \n",
       "219        662     30           23  Crystal Blue-Haired Persuasion   \n",
       "\n",
       "                         airdate viewers (millions)  \n",
       "217  April 28, 2019 (2019-04-28)               1.93  \n",
       "218     May 5, 2019 (2019-05-05)               1.79  \n",
       "219    May 12, 2019 (2019-05-12)               1.50  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinating them together will create the final dataframe for the episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes = pd.concat([df_1,df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes.reset_index(inplace=True)\n",
    "df_episodes.drop('index',axis=1,inplace=True)\n",
    "\n",
    "df_episodes['no_overall'] = df_episodes['no_overall'].apply(lambda x: int(x))\n",
    "df_episodes['season'] = df_episodes['season'].apply(lambda x: int(x))\n",
    "df_episodes['no_in_season'] = df_episodes['no_in_season'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to clean **season 28**, here episode **607**, are two seperate episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_overall                                   608609\n",
       "season                                           28\n",
       "no_in_season                                   1213\n",
       "episode_name          The Great Phatsby Parts 1 & 2\n",
       "airdate               January 15, 2017 (2017-01-15)\n",
       "viewers (millions)                             6.90\n",
       "Name: 607, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes.iloc[607]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the episode up into episode **608** and **609** will make the data make more sense, as the overall episode count will be linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes.drop(607,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into = pd.DataFrame(data = [{'no_overall':608, 'season':28,'no_in_season': 12, \n",
    "                            'episode_name':'The Great Phatsby','airdate': 'January 15, 2017 (2017-01-15)', \n",
    "                            'viewers (millions)': 6.90}, {'no_overall':609, 'season':28,'no_in_season': 13, \n",
    "                            'episode_name':'The Great Phatsby','airdate': 'January 15, 2017 (2017-01-15)', \n",
    "                            'viewers (millions)': 6.90}], index=[607,608])\n",
    "\n",
    "df_episodes = pd.concat([df_episodes.iloc[:608], insert_into, df_episodes[608:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Ratings <a class=\"anchor\" id=\"two.one.two\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the ratings for each episode is possible by scraping IMDB. There exist several pages of ratings for the series on IMDB. Going through each one of them will yield every episode's rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_ratings_url = 'https://www.imdb.com/list/ls052175596/?st_dt=&mode=simple&page={}&ref_=ttls_vw_smp&sort=release_date,asc'\n",
    "pages = range(1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = []\n",
    "for page in range(1,8):\n",
    "    url = imdb_ratings_url.format(page)\n",
    "    res = requests.get(url).text\n",
    "    soup = BeautifulSoup(res,'html')\n",
    "    container = soup.find('div',class_='lister')\n",
    "    for item in container.find_all('div', class_='lister-item'):\n",
    "        as_ = item.find('span',class_='lister-item-header').find_all('a')\n",
    "        episode_name = as_[1].text\n",
    "        rating = item.find('div',class_='col-imdb-rating').text.strip()\n",
    "        ratings.append([episode_name,rating])\n",
    "\n",
    "df_ratings = pd.DataFrame(ratings)\n",
    "df_ratings.columns = ['episode_name','IMDB rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>Hail to the Teeth</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>The Miseducation of Lisa Simpson</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Frinkcoin</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>Bart the Bad Guy</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>Screenless</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>Better Off Ned</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>Highway to Well</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>The Incredible Lightness of Being a Baby</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>Warrin' Priests</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>Warrin' Priests Part 2</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 episode_name IMDB rating\n",
       "671                         Hail to the Teeth           6\n",
       "672          The Miseducation of Lisa Simpson         6.6\n",
       "673                                 Frinkcoin         5.9\n",
       "674                          Bart the Bad Guy         7.3\n",
       "675                                Screenless         6.6\n",
       "676                            Better Off Ned         6.5\n",
       "677                           Highway to Well           7\n",
       "678  The Incredible Lightness of Being a Baby         6.4\n",
       "679                           Warrin' Priests           6\n",
       "680                    Warrin' Priests Part 2         5.6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular project, only the first 660 episodes will be investigated. This decision was made as the last episodes do not have as much description written about them as they are quite new. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_ratings[:661]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>Woo-Hoo Dunnit?</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>Crystal Blue-Haired Persuasion</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       episode_name IMDB rating\n",
       "659                 Woo-Hoo Dunnit?         5.9\n",
       "660  Crystal Blue-Haired Persuasion         5.8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinating these two DataFrames will yield more informative data. However, a simple merge on the names will not do because the names are not exactly identical on IMDB and Wikipedia. The `NaN` values will be looked at and manually fixed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings = pd.merge(df_episodes, df_ratings, on='episode_name',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following episodes do not get a rating after the join because the naming convention are different between IMDB and Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Bart Gets an \"F\"</td>\n",
       "      <td>October 11, 1990 (1990-10-11)</td>\n",
       "      <td>33.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>Itchy &amp; Scratchy &amp; Marge</td>\n",
       "      <td>December 20, 1990 (1990-12-20)</td>\n",
       "      <td>22.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Bart's Dog Gets an \"F\"</td>\n",
       "      <td>March 7, 1991 (1991-03-07)</td>\n",
       "      <td>23.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>Boy-Scoutz 'n the Hood</td>\n",
       "      <td>November 18, 1993 (1993-11-18)</td>\n",
       "      <td>20.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>$pringfield (or, How I Learned to Stop Worryin...</td>\n",
       "      <td>December 16, 1993 (1993-12-16)</td>\n",
       "      <td>17.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>124</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>The PTA Disbands!</td>\n",
       "      <td>April 16, 1995 (1995-04-16)</td>\n",
       "      <td>11.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>Who Shot Mr. Burns? (Part One)</td>\n",
       "      <td>May 21, 1995 (1995-05-21)</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>129</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Who Shot Mr. Burns? (Part Two)</td>\n",
       "      <td>September 17, 1995 (1995-09-17)</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>131</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>Home Sweet Homediddly-Dum-Doodily</td>\n",
       "      <td>October 1, 1995 (1995-10-01)</td>\n",
       "      <td>14.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>135</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>King-Size Homer</td>\n",
       "      <td>November 5, 1995 (1995-11-05)</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>138</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>The Simpsons 138th Episode Spectacular</td>\n",
       "      <td>December 3, 1995 (1995-12-03)</td>\n",
       "      <td>16.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>Raging Abe Simpson and His Grumbling Grandson ...</td>\n",
       "      <td>April 28, 1996 (1996-04-28)</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>153</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>Summer of 4 Ft. 2</td>\n",
       "      <td>May 19, 1996 (1996-05-19)</td>\n",
       "      <td>14.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>162</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>El Viaje Misterioso de Nuestro Jomer (The Myst...</td>\n",
       "      <td>January 5, 1997 (1997-01-05)</td>\n",
       "      <td>14.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>171</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>Homer vs. the Eighteenth Amendment</td>\n",
       "      <td>March 16, 1997 (1997-03-16)</td>\n",
       "      <td>14.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>210</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>Lisa Gets an \"A\"</td>\n",
       "      <td>November 22, 1998 (1998-11-22)</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>211</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>Homer Simpson in: \"Kidney Trouble\"</td>\n",
       "      <td>December 6, 1998 (1998-12-06)</td>\n",
       "      <td>7.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>218</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>Marge Simpson in: \"Screaming Yellow Honkers\"</td>\n",
       "      <td>February 21, 1999 (1999-02-21)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>223</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>The Old Man and the \"C\" Student</td>\n",
       "      <td>April 25, 1999 (1999-04-25)</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>226</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>Thirty Minutes over Tokyo</td>\n",
       "      <td>May 16, 1999 (1999-05-16)</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>240</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>Alone Again, Natura-Diddily</td>\n",
       "      <td>February 13, 2000 (2000-02-13)</td>\n",
       "      <td>10.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>252</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>Lisa the Tree Hugger</td>\n",
       "      <td>November 19, 2000 (2000-11-19)</td>\n",
       "      <td>14.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>257</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>HOMR</td>\n",
       "      <td>January 7, 2001 (2001-01-07)</td>\n",
       "      <td>18.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>263</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>Hungry, Hungry Homer</td>\n",
       "      <td>March 4, 2001 (2001-03-04)</td>\n",
       "      <td>17.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>264</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>Bye Bye Nerdie</td>\n",
       "      <td>March 11, 2001 (2001-03-11)</td>\n",
       "      <td>16.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>287</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>I Am Furious (Yellow)</td>\n",
       "      <td>April 28, 2002 (2002-04-28)</td>\n",
       "      <td>12.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>291</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>Poppa's Got a Brand New Badge</td>\n",
       "      <td>May 22, 2002 (2002-05-22)</td>\n",
       "      <td>8.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>294</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>Bart vs. Lisa vs. the Third Grade</td>\n",
       "      <td>November 17, 2002 (2002-11-17)</td>\n",
       "      <td>13.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>319</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>Today I Am a Clown</td>\n",
       "      <td>December 7, 2003 (2003-12-07)</td>\n",
       "      <td>10.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>322</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>I, (Annoyed Grunt)-bot</td>\n",
       "      <td>January 11, 2004 (2004-01-11)</td>\n",
       "      <td>16.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>328</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Co-Dependents' Day</td>\n",
       "      <td>March 21, 2004 (2004-03-21)</td>\n",
       "      <td>11.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>354</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>Thank God, It's Doomsday</td>\n",
       "      <td>May 8, 2005 (2005-05-08)</td>\n",
       "      <td>10.05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>378</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>Marge and Homer Turn a Couple Play</td>\n",
       "      <td>May 21, 2006 (2006-05-21)</td>\n",
       "      <td>8.23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>380</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>Jazzy and the Pussycats</td>\n",
       "      <td>September 17, 2006 (2006-09-17)</td>\n",
       "      <td>8.94</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>385</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>Ice Cream of Margie (with the Light Blue Hair)</td>\n",
       "      <td>November 26, 2006 (2006-11-26)</td>\n",
       "      <td>10.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>387</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>Kill Gil, Volumes I &amp; II</td>\n",
       "      <td>December 17, 2006 (2006-12-17)</td>\n",
       "      <td>8.96</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>393</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>Rome-Old and Juli-Eh</td>\n",
       "      <td>March 11, 2007 (2007-03-11)</td>\n",
       "      <td>8.98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>Stop! Or My Dog Will Shoot</td>\n",
       "      <td>May 13, 2007 (2007-05-13)</td>\n",
       "      <td>6.48</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>402</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>The Homer of Seville</td>\n",
       "      <td>September 30, 2007 (2007-09-30)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>413</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>The Debarted</td>\n",
       "      <td>March 2, 2008 (2008-03-02)</td>\n",
       "      <td>8.18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>414</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>Dial \"N\" for Nerder</td>\n",
       "      <td>March 9, 2008 (2008-03-09)</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>439</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>Waverly Hills, 9-0-2-1-D'oh</td>\n",
       "      <td>May 3, 2009 (2009-05-03)</td>\n",
       "      <td>6.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>449</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>O Brother, Where Bart Thou?</td>\n",
       "      <td>December 13, 2009 (2009-12-13)</td>\n",
       "      <td>7.11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>461</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>To Surveil with Love</td>\n",
       "      <td>May 2, 2010 (2010-05-02)</td>\n",
       "      <td>6.06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>480</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>A Midsummer's Nice Dream</td>\n",
       "      <td>March 13, 2011 (2011-03-13)</td>\n",
       "      <td>5.44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>487</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>The Falcon and the D'ohman</td>\n",
       "      <td>September 25, 2011 (2011-09-25)</td>\n",
       "      <td>8.08</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>505</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>A Totally Fun Thing That Bart Will Never Do Again</td>\n",
       "      <td>April 29, 2012 (2012-04-29)</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>507</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>Ned 'n' Edna's Blend Agenda</td>\n",
       "      <td>May 13, 2012 (2012-05-13)</td>\n",
       "      <td>4.07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>519</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>The Changing of the Guardian</td>\n",
       "      <td>January 27, 2013 (2013-01-27)</td>\n",
       "      <td>5.23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>520</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>Love Is a Many-Splintered Thing</td>\n",
       "      <td>February 10, 2013 (2013-02-10)</td>\n",
       "      <td>4.19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>523</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>Black Eyed, Please</td>\n",
       "      <td>March 10, 2013 (2013-03-10)</td>\n",
       "      <td>4.85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>576</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>Cue Detective</td>\n",
       "      <td>October 4, 2015 (2015-10-04)</td>\n",
       "      <td>6.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>581</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>Lisa with an 'S'</td>\n",
       "      <td>November 22, 2015 (2015-11-22)</td>\n",
       "      <td>5.64</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>601</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>Trust but Clarify</td>\n",
       "      <td>October 23, 2016 (2016-10-23)</td>\n",
       "      <td>3.36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>624</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>The Old Blue Mayor She Ain't What She Used to Be</td>\n",
       "      <td>November 12, 2017 (2017-11-12)</td>\n",
       "      <td>4.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>630</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>Homer Is Where the Art Isn't</td>\n",
       "      <td>March 18, 2018 (2018-03-18)</td>\n",
       "      <td>2.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall  season  no_in_season  \\\n",
       "13           14       2             1   \n",
       "21           22       2             9   \n",
       "28           29       2            16   \n",
       "88           89       5             8   \n",
       "90           91       5            10   \n",
       "123         124       6            21   \n",
       "127         128       6            25   \n",
       "128         129       7             1   \n",
       "130         131       7             3   \n",
       "134         135       7             7   \n",
       "137         138       7            10   \n",
       "149         150       7            22   \n",
       "152         153       7            25   \n",
       "161         162       8             9   \n",
       "170         171       8            18   \n",
       "209         210      10             7   \n",
       "210         211      10             8   \n",
       "217         218      10            15   \n",
       "222         223      10            20   \n",
       "225         226      10            23   \n",
       "239         240      11            14   \n",
       "251         252      12             4   \n",
       "256         257      12             9   \n",
       "262         263      12            15   \n",
       "263         264      12            16   \n",
       "286         287      13            18   \n",
       "290         291      13            22   \n",
       "293         294      14             3   \n",
       "318         319      15             6   \n",
       "321         322      15             9   \n",
       "327         328      15            15   \n",
       "353         354      16            19   \n",
       "377         378      17            22   \n",
       "379         380      18             2   \n",
       "384         385      18             7   \n",
       "386         387      18             9   \n",
       "392         393      18            15   \n",
       "397         398      18            20   \n",
       "401         402      19             2   \n",
       "412         413      19            13   \n",
       "413         414      19            14   \n",
       "438         439      20            19   \n",
       "448         449      21             8   \n",
       "460         461      21            20   \n",
       "479         480      22            16   \n",
       "486         487      23             1   \n",
       "504         505      23            19   \n",
       "506         507      23            21   \n",
       "518         519      24            11   \n",
       "519         520      24            12   \n",
       "522         523      24            15   \n",
       "575         576      27             2   \n",
       "580         581      27             7   \n",
       "600         601      28             5   \n",
       "623         624      29             6   \n",
       "629         630      29            12   \n",
       "\n",
       "                                          episode_name  \\\n",
       "13                                    Bart Gets an \"F\"   \n",
       "21                            Itchy & Scratchy & Marge   \n",
       "28                              Bart's Dog Gets an \"F\"   \n",
       "88                              Boy-Scoutz 'n the Hood   \n",
       "90   $pringfield (or, How I Learned to Stop Worryin...   \n",
       "123                                  The PTA Disbands!   \n",
       "127                     Who Shot Mr. Burns? (Part One)   \n",
       "128                     Who Shot Mr. Burns? (Part Two)   \n",
       "130                  Home Sweet Homediddly-Dum-Doodily   \n",
       "134                                    King-Size Homer   \n",
       "137             The Simpsons 138th Episode Spectacular   \n",
       "149  Raging Abe Simpson and His Grumbling Grandson ...   \n",
       "152                                  Summer of 4 Ft. 2   \n",
       "161  El Viaje Misterioso de Nuestro Jomer (The Myst...   \n",
       "170                 Homer vs. the Eighteenth Amendment   \n",
       "209                                   Lisa Gets an \"A\"   \n",
       "210                 Homer Simpson in: \"Kidney Trouble\"   \n",
       "217       Marge Simpson in: \"Screaming Yellow Honkers\"   \n",
       "222                    The Old Man and the \"C\" Student   \n",
       "225                          Thirty Minutes over Tokyo   \n",
       "239                        Alone Again, Natura-Diddily   \n",
       "251                               Lisa the Tree Hugger   \n",
       "256                                               HOMR   \n",
       "262                               Hungry, Hungry Homer   \n",
       "263                                     Bye Bye Nerdie   \n",
       "286                              I Am Furious (Yellow)   \n",
       "290                      Poppa's Got a Brand New Badge   \n",
       "293                  Bart vs. Lisa vs. the Third Grade   \n",
       "318                                 Today I Am a Clown   \n",
       "321                             I, (Annoyed Grunt)-bot   \n",
       "327                                 Co-Dependents' Day   \n",
       "353                           Thank God, It's Doomsday   \n",
       "377                 Marge and Homer Turn a Couple Play   \n",
       "379                            Jazzy and the Pussycats   \n",
       "384     Ice Cream of Margie (with the Light Blue Hair)   \n",
       "386                           Kill Gil, Volumes I & II   \n",
       "392                               Rome-Old and Juli-Eh   \n",
       "397                         Stop! Or My Dog Will Shoot   \n",
       "401                               The Homer of Seville   \n",
       "412                                       The Debarted   \n",
       "413                                Dial \"N\" for Nerder   \n",
       "438                        Waverly Hills, 9-0-2-1-D'oh   \n",
       "448                        O Brother, Where Bart Thou?   \n",
       "460                               To Surveil with Love   \n",
       "479                           A Midsummer's Nice Dream   \n",
       "486                         The Falcon and the D'ohman   \n",
       "504  A Totally Fun Thing That Bart Will Never Do Again   \n",
       "506                        Ned 'n' Edna's Blend Agenda   \n",
       "518                       The Changing of the Guardian   \n",
       "519                    Love Is a Many-Splintered Thing   \n",
       "522                                 Black Eyed, Please   \n",
       "575                                      Cue Detective   \n",
       "580                                   Lisa with an 'S'   \n",
       "600                                  Trust but Clarify   \n",
       "623   The Old Blue Mayor She Ain't What She Used to Be   \n",
       "629                       Homer Is Where the Art Isn't   \n",
       "\n",
       "                             airdate viewers (millions) IMDB rating  \n",
       "13     October 11, 1990 (1990-10-11)               33.6         NaN  \n",
       "21    December 20, 1990 (1990-12-20)               22.2         NaN  \n",
       "28        March 7, 1991 (1991-03-07)               23.9         NaN  \n",
       "88    November 18, 1993 (1993-11-18)               20.1         NaN  \n",
       "90    December 16, 1993 (1993-12-16)               17.9         NaN  \n",
       "123      April 16, 1995 (1995-04-16)               11.8         NaN  \n",
       "127        May 21, 1995 (1995-05-21)               15.0         NaN  \n",
       "128  September 17, 1995 (1995-09-17)               16.0         NaN  \n",
       "130     October 1, 1995 (1995-10-01)               14.5         NaN  \n",
       "134    November 5, 1995 (1995-11-05)               17.0         NaN  \n",
       "137    December 3, 1995 (1995-12-03)               16.4         NaN  \n",
       "149      April 28, 1996 (1996-04-28)               13.0         NaN  \n",
       "152        May 19, 1996 (1996-05-19)               14.7         NaN  \n",
       "161     January 5, 1997 (1997-01-05)               14.9         NaN  \n",
       "170      March 16, 1997 (1997-03-16)               14.6         NaN  \n",
       "209   November 22, 1998 (1998-11-22)                  8         NaN  \n",
       "210    December 6, 1998 (1998-12-06)                7.2         NaN  \n",
       "217   February 21, 1999 (1999-02-21)                8.6         NaN  \n",
       "222      April 25, 1999 (1999-04-25)                6.9         NaN  \n",
       "225        May 16, 1999 (1999-05-16)                  8         NaN  \n",
       "239   February 13, 2000 (2000-02-13)               10.8         NaN  \n",
       "251   November 19, 2000 (2000-11-19)               14.9         NaN  \n",
       "256     January 7, 2001 (2001-01-07)               18.5         NaN  \n",
       "262       March 4, 2001 (2001-03-04)               17.6         NaN  \n",
       "263      March 11, 2001 (2001-03-11)               16.1         NaN  \n",
       "286      April 28, 2002 (2002-04-28)               12.4         NaN  \n",
       "290        May 22, 2002 (2002-05-22)                8.2         NaN  \n",
       "293   November 17, 2002 (2002-11-17)               13.3         NaN  \n",
       "318    December 7, 2003 (2003-12-07)               10.5         NaN  \n",
       "321    January 11, 2004 (2004-01-11)               16.3         NaN  \n",
       "327      March 21, 2004 (2004-03-21)               11.2         NaN  \n",
       "353         May 8, 2005 (2005-05-08)              10.05         NaN  \n",
       "377        May 21, 2006 (2006-05-21)               8.23         NaN  \n",
       "379  September 17, 2006 (2006-09-17)               8.94         NaN  \n",
       "384   November 26, 2006 (2006-11-26)              10.90         NaN  \n",
       "386   December 17, 2006 (2006-12-17)               8.96         NaN  \n",
       "392      March 11, 2007 (2007-03-11)               8.98         NaN  \n",
       "397        May 13, 2007 (2007-05-13)               6.48         NaN  \n",
       "401  September 30, 2007 (2007-09-30)                8.4         NaN  \n",
       "412       March 2, 2008 (2008-03-02)               8.18         NaN  \n",
       "413       March 9, 2008 (2008-03-09)                7.3         NaN  \n",
       "438         May 3, 2009 (2009-05-03)               6.75         NaN  \n",
       "448   December 13, 2009 (2009-12-13)               7.11         NaN  \n",
       "460         May 2, 2010 (2010-05-02)               6.06         NaN  \n",
       "479      March 13, 2011 (2011-03-13)               5.44         NaN  \n",
       "486  September 25, 2011 (2011-09-25)               8.08         NaN  \n",
       "504      April 29, 2012 (2012-04-29)               5.00         NaN  \n",
       "506        May 13, 2012 (2012-05-13)               4.07         NaN  \n",
       "518    January 27, 2013 (2013-01-27)               5.23         NaN  \n",
       "519   February 10, 2013 (2013-02-10)               4.19         NaN  \n",
       "522      March 10, 2013 (2013-03-10)               4.85         NaN  \n",
       "575     October 4, 2015 (2015-10-04)               6.02         NaN  \n",
       "580   November 22, 2015 (2015-11-22)               5.64         NaN  \n",
       "600    October 23, 2016 (2016-10-23)               3.36         NaN  \n",
       "623   November 12, 2017 (2017-11-12)               4.75         NaN  \n",
       "629      March 18, 2018 (2018-03-18)               2.10         NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings[df_episodes_ratings['IMDB rating'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of the names of the episodes that are missing their ratings can be created: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bart Gets an F 8.2\n",
      "Itchy and Scratchy and Marge 8.1\n",
      "Bart's Dog Gets an F 7.5\n",
      "Boy Scoutz 'n the Hood 8.7\n",
      "$pringfield (Or, How I Learned to Stop Worrying and Love Legalized Gambling) 8.6\n",
      "The PTA Disbands 8.1\n",
      "Who Shot Mr. Burns? Part One 9.2\n",
      "Who Shot Mr. Burns? Part Two 9\n",
      "Home Sweet Home-Dum-Diddly Doodily 8.4\n",
      "King Size Homer 9\n",
      "The Simpsons 138th Episode Spectacular! 7.5\n",
      "Raging Abe Simpson and His Grumbling Grandson in 'The Curse of the Flying Hellfish' 8.4\n",
      "Summer of 4'2\" 8.4\n",
      "El Viaje Misterioso De Nuestro Jomer 8.6\n",
      "Homer vs. the 18th Amendment 8.9\n",
      "Lisa Gets an 'A' 8.1\n",
      "Homer Simpson in: 'Kidney Trouble' 7.3\n",
      "Marge Simpson in 'Screaming Yellow Honkers' 7.3\n",
      "The Old Man and the 'C' Student 7.3\n",
      "Thirty Minutes Over Tokyo 8\n",
      "Alone Again, Natura-Diddly 7.7\n",
      "Lisa the Treehugger 7.1\n",
      "Homr 8.1\n",
      "Hungry Hungry Homer 7.5\n",
      "Bye Bye Nerdy 6.6\n",
      "I Am Furious Yellow 7.7\n",
      "Papa's Got a Brand New Badge 7.8\n",
      "Bart vs. Lisa vs. 3rd Grade 7.1\n",
      "Today, I Am a Clown 6.4\n",
      "I, (Annoyed Grunt)-Bot 7.2\n",
      "Co-Dependent's Day 6.7\n",
      "Thank God It's Doomsday 7.2\n",
      "Homer and Marge Turn a Couple Play 6.3\n",
      "Jazzy & The Pussycats 6.6\n",
      "Ice Cream of Margie: With the Light Blue Hair 6.9\n",
      "Kill Gil, Vol. 1 & 2 6.2\n",
      "Rome-old and Juli-eh 6.1\n",
      "Stop or My Dog Will Shoot 6.7\n",
      "Homer of Seville 6.5\n",
      "The DeBarted 7.6\n",
      "Dial 'N' for Nerder 7.3\n",
      "Waverly Hills, 9021-D'Oh 7\n",
      "Oh Brother, Where Bart Thou? 7.1\n",
      "To Surveil, with Love 7\n",
      "A Midsummer's Nice Dreams 5.9\n",
      "The Falcon and the D'Ohman 7\n",
      "A Totally Fun Thing Bart Will Never Do Again 7.5\n",
      "Ned 'N' Edna's Blend 6.5\n",
      "Changing of the Guardian 6.3\n",
      "Love Is a Many Splintered Thing 6.1\n",
      "Black-Eyed, Please 6.8\n",
      "'Cue Detective 6.6\n",
      "Lisa with an \"S\" 5.9\n",
      "Trust But Clarify 6.3\n",
      "Singin' in the Lane 6.3\n",
      "3 Scenes Plus a Tag from a Marriage 6.6\n"
     ]
    }
   ],
   "source": [
    "missing = df_episodes_ratings[df_episodes_ratings['IMDB rating'].isna()].reset_index()\n",
    "missing_episodes = missing['index']\n",
    "for episode in missing_episodes:\n",
    "    print(df_ratings.iloc[episode]['episode_name'],df_ratings.iloc[episode]['IMDB rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen by comparing the two lists, these are the episodes that are missing a rating in the final dataframe. Now, an array can be created with the ratings of these episodes, extracted from `df_ratings`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ratings = []\n",
    "for episode in missing_episodes:\n",
    "    missing_ratings.append([episode,df_episodes.iloc[episode]['episode_name'],df_ratings.iloc[episode]['IMDB rating']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in missing_ratings: \n",
    "    index = episode[0]\n",
    "    rating = episode[2]\n",
    "    df_episodes_ratings.at[index, 'IMDB rating'] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [no_overall, season, no_in_season, episode_name, airdate, viewers (millions), IMDB rating]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings[df_episodes_ratings['IMDB rating'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame holding each episode's information along with it's rating is now complete, but the airdate column can use a little bit of cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>December 17, 1989 (1989-12-17)</td>\n",
       "      <td>26.7</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart the Genius</td>\n",
       "      <td>January 14, 1990 (1990-01-14)</td>\n",
       "      <td>24.5</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_overall  season  no_in_season                       episode_name  \\\n",
       "0           1       1             1  Simpsons Roasting on an Open Fire   \n",
       "1           2       1             2                    Bart the Genius   \n",
       "\n",
       "                          airdate viewers (millions) IMDB rating  \n",
       "0  December 17, 1989 (1989-12-17)               26.7         8.2  \n",
       "1   January 14, 1990 (1990-01-14)               24.5         7.7  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings['year'] = df_episodes_ratings['airdate'].apply(lambda x: str(x).split(\"(\")[1].split(\"-\")[0])\n",
    "df_episodes_ratings['month'] = df_episodes_ratings['airdate'].apply(lambda x: str(x).split(\"(\")[1].split(\"-\")[1].split('-')[0])\n",
    "df_episodes_ratings['day'] = df_episodes_ratings['airdate'].apply(lambda x: str(x).split(\"(\")[1].split(\"-\")[2].replace(\")\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>December 17, 1989 (1989-12-17)</td>\n",
       "      <td>26.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1989</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart the Genius</td>\n",
       "      <td>January 14, 1990 (1990-01-14)</td>\n",
       "      <td>24.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1990</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_overall  season  no_in_season                       episode_name  \\\n",
       "0           1       1             1  Simpsons Roasting on an Open Fire   \n",
       "1           2       1             2                    Bart the Genius   \n",
       "\n",
       "                          airdate viewers (millions) IMDB rating  year month  \\\n",
       "0  December 17, 1989 (1989-12-17)               26.7         8.2  1989    12   \n",
       "1   January 14, 1990 (1990-01-14)               24.5         7.7  1990    01   \n",
       "\n",
       "  day  \n",
       "0  17  \n",
       "1  14  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Airdate column can now be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings.drop('airdate',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the IMDB rating column should hold float values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings['IMDB rating'] = df_episodes_ratings['IMDB rating'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some `NaN` values in the column `viewers (millions)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>160</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>Lisa's Date with Density</td>\n",
       "      <td>N/A</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1996</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>161</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Hurricane Neddy</td>\n",
       "      <td>N/A</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1996</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>173</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>The Canine Mutiny</td>\n",
       "      <td>N/A</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1997</td>\n",
       "      <td>04</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall  season  no_in_season              episode_name  \\\n",
       "159         160       8             7  Lisa's Date with Density   \n",
       "160         161       8             8           Hurricane Neddy   \n",
       "172         173       8            20         The Canine Mutiny   \n",
       "\n",
       "    viewers (millions)  IMDB rating  year month day  \n",
       "159                N/A          7.8  1996    12  15  \n",
       "160                N/A          8.8  1996    12  29  \n",
       "172                N/A          7.6  1997    04  13  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings[df_episodes_ratings['viewers (millions)'] == 'N/A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values will be set as the average of the viewers for the season that they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_season_8 = np.average([float(viewer) for viewer in df_episodes_ratings[(df_episodes_ratings['viewers (millions)'] != 'N/A') & (df_episodes_ratings['season'] == 8)]['viewers (millions)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values that need to be filled in are in places 159, 160 and 172:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na_indices = [index for index in df_episodes_ratings[df_episodes_ratings['viewers (millions)'] == 'N/A'].reset_index()['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in fill_na_indices:    \n",
    "    df_episodes_ratings.at[index,'viewers (millions)'] = avg_season_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column should also hold float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings['viewers (millions)'] = df_episodes_ratings['viewers (millions)'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_overall            0\n",
       "season                0\n",
       "no_in_season          0\n",
       "episode_name          0\n",
       "viewers (millions)    0\n",
       "IMDB rating           0\n",
       "year                  0\n",
       "month                 0\n",
       "day                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings.to_csv('episode_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpsons.fandom Pages of Each Episode <a class=\"anchor\" id=\"two.one.four\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scour through each episode's description, scraping a Wiki fan-page of the series is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_url = 'https://simpsons.fandom.com/wiki/List_of_Episodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_array = []\n",
    "res = requests.get(wiki_url).text\n",
    "soup = BeautifulSoup(res,'html')\n",
    "no_overall = 1\n",
    "for table in soup.find_all('table', class_='wikitable')[1:31]: #Skip the overview table and only take the first 30 seasons\n",
    "    for row in table.find_all('tr'):\n",
    "            links = row.find_all('a')\n",
    "            for link in links:\n",
    "                if(link['href'].split('/wiki/')[1] != 'The_Simpsons_Movie'):\n",
    "                    links_array.append([no_overall,link['href'].split('/wiki/')[1]])\n",
    "                    no_overall = no_overall + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episode_wiki = pd.DataFrame(links_array)\n",
    "df_episode_wiki.columns = ['no_overall','wiki_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>wiki_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons_Roasting_on_an_Open_Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bart_the_Genius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Homer%27s_Odyssey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>There%27s_No_Disgrace_Like_Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bart_the_General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>657</td>\n",
       "      <td>Girl%27s_in_the_Band</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>658</td>\n",
       "      <td>I%27m_Just_a_Girl_Who_Can%27t_Say_D%27oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>659</td>\n",
       "      <td>D%27oh_Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>660</td>\n",
       "      <td>Woo-hoo_Dunnit%3F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>661</td>\n",
       "      <td>Crystal_Blue-Haired_Persuasion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>661 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall                                 wiki_link\n",
       "0             1         Simpsons_Roasting_on_an_Open_Fire\n",
       "1             2                           Bart_the_Genius\n",
       "2             3                         Homer%27s_Odyssey\n",
       "3             4           There%27s_No_Disgrace_Like_Home\n",
       "4             5                          Bart_the_General\n",
       "..          ...                                       ...\n",
       "656         657                      Girl%27s_in_the_Band\n",
       "657         658  I%27m_Just_a_Girl_Who_Can%27t_Say_D%27oh\n",
       "658         659                             D%27oh_Canada\n",
       "659         660                         Woo-hoo_Dunnit%3F\n",
       "660         661            Crystal_Blue-Haired_Persuasion\n",
       "\n",
       "[661 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episode_wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the episode 'The Great Phatsby' (episode 608/609), is causing problems. It can be solved as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "phatsby_insert = pd.DataFrame({'no_overall':609, 'wiki_link':'The_Great_Phatsby'},index=[608])\n",
    "phatsby_insert\n",
    "df_episode_wiki = pd.concat([df_episode_wiki.iloc[:608], phatsby_insert, df_episode_wiki[608:]])\n",
    "df_episode_wiki.reset_index(inplace=True, drop=True)\n",
    "df_episode_wiki_no_correction = df_episode_wiki.iloc[609:]\n",
    "df_episode_wiki_no_correction['no_overall'] = df_episode_wiki_no_correction['no_overall'].apply(lambda x: x+1)\n",
    "df_episde_wiki = pd.concat([df_episode_wiki.iloc[:608], df_episode_wiki_no_correction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings = pd.merge(df_episodes_ratings,df_episode_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>wiki_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>26.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1989</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>Simpsons_Roasting_on_an_Open_Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart the Genius</td>\n",
       "      <td>24.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1990</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>Bart_the_Genius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer's Odyssey</td>\n",
       "      <td>27.5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1990</td>\n",
       "      <td>01</td>\n",
       "      <td>21</td>\n",
       "      <td>Homer%27s_Odyssey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>There's No Disgrace Like Home</td>\n",
       "      <td>20.2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1990</td>\n",
       "      <td>01</td>\n",
       "      <td>28</td>\n",
       "      <td>There%27s_No_Disgrace_Like_Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Bart the General</td>\n",
       "      <td>27.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1990</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>Bart_the_General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_overall  season  no_in_season                       episode_name  \\\n",
       "0           1       1             1  Simpsons Roasting on an Open Fire   \n",
       "1           2       1             2                    Bart the Genius   \n",
       "2           3       1             3                    Homer's Odyssey   \n",
       "3           4       1             4      There's No Disgrace Like Home   \n",
       "4           5       1             5                   Bart the General   \n",
       "\n",
       "   viewers (millions)  IMDB rating  year month day  \\\n",
       "0                26.7          8.2  1989    12  17   \n",
       "1                24.5          7.7  1990    01  14   \n",
       "2                27.5          7.4  1990    01  21   \n",
       "3                20.2          7.7  1990    01  28   \n",
       "4                27.1          8.0  1990    02  04   \n",
       "\n",
       "                           wiki_link  \n",
       "0  Simpsons_Roasting_on_an_Open_Fire  \n",
       "1                    Bart_the_Genius  \n",
       "2                  Homer%27s_Odyssey  \n",
       "3    There%27s_No_Disgrace_Like_Home  \n",
       "4                   Bart_the_General  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Text Files of Episodes <a class=\"anchor\" id=\"two.one.five\"></a>\n",
    "\n",
    "Here, each description is fetched and saved as a text file in order to attach them to the data set later. Below, a short description is fetched from another section of each episode's wiki website. The following image explains the process. The upper description of the episode is much shorter and will be scraped separately. The latter is a much longer, thorough description and will be scraped below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scrape](scrape.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('synopsis'):\n",
    "        os.mkdir('synopsis')\n",
    "        \n",
    "not_full_story = ['Mr._Spritz_Goes_to_Washington', \n",
    "                  'Three_Gays_of_the_Condo', \n",
    "                  'The_Fat_and_the_Furriest', \n",
    "                  'Today,_I_Am_a_Clown', \n",
    "                  'Simple_Simpson', \n",
    "                  'Fat_Man_and_Little_Boy', \n",
    "                  'My_Fare_Lady', \n",
    "                  'To_Courier_with_Love', \n",
    "                  'Lisa_Gets_the_Blues', \n",
    "                  'Girl%27s_in_the_Band',\n",
    "                  'A_Star_is_Born-Again',\n",
    "                  'Milhouse_Doesn%27t_Live_Here_Anymore']\n",
    "\n",
    "for season in df_episodes_ratings['season'].unique():\n",
    "    if not os.path.exists('synopsis/'+'season'+str(season)):\n",
    "        os.mkdir('synopsis/'+'season'+str(season))\n",
    "\n",
    "save_path = os.getcwd()+'/synopsis/'\n",
    "for episode in df_episodes_ratings['wiki_link']:\n",
    "    syn = \" \"\n",
    "    \n",
    "    res = requests.get(\"https://simpsons.fandom.com/wiki/{}\".format(episode)).text\n",
    "    ep_number = df_episodes_ratings[df_episodes_ratings['wiki_link']==episode]['no_overall'].values[0]\n",
    "    season = int(df_episodes_ratings[df_episodes_ratings['wiki_link']==episode]['season'].values[0])\n",
    "    \n",
    "    soup = BeautifulSoup(res,'html')\n",
    "    \n",
    "    # Remove all images\n",
    "    figs = soup.findAll('figure')\n",
    "    for fig in figs:\n",
    "        fig.decompose()\n",
    "        \n",
    "    h3s = soup.findAll('h3')\n",
    "    for h3 in h3s:\n",
    "        h3.decompose()\n",
    "\n",
    "    if(episode in not_full_story):\n",
    "        parent = soup.find('span', {'id':'Synopsis'}).parent\n",
    "        sibling = parent.next_sibling\n",
    "        while(sibling.name == 'p' or sibling.name is None or sibling.name =='a' or sibling.name==''):\n",
    "            if(sibling.name == 'p' or sibling.name=='a'):\n",
    "                syn += ' ' + str(sibling.text)\n",
    "                sibling = sibling.next_sibling\n",
    "            else:\n",
    "                sibling = sibling.next_sibling\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if(soup.find('span', {'id':'Full_Story'})):\n",
    "            \n",
    "            if(episode=='HOM%D0%AF'):\n",
    "                parent = soup.find('span', {'id':'Full_Story'}).parent\n",
    "                sibling = parent.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling\n",
    "                while(sibling.name == 'p' or sibling.name is None or sibling.name =='a' or sibling.name=='i'):\n",
    "\n",
    "                    if(sibling.name == 'p' or sibling.name=='a' or sibling.name=='i'):\n",
    "                        syn += ' ' + str(sibling.text)\n",
    "                        sibling = sibling.next_sibling\n",
    "                    else:\n",
    "                        sibling = sibling.next_sibling\n",
    "            if(episode=='I,_(Annoyed_Grunt)-Bot'):\n",
    "                parent = soup.find('span', {'id':'Full_Story'}).parent\n",
    "                sibling = parent.next_sibling.next_sibling.next_sibling.next_sibling\n",
    "                while(sibling.name == 'p' or sibling.name is None or sibling.name =='a' or sibling.name=='i'):\n",
    "\n",
    "                    if(sibling.name == 'p' or sibling.name=='a' or sibling.name=='i'):\n",
    "                        syn += ' ' + str(sibling.text)\n",
    "                        sibling = sibling.next_sibling\n",
    "                    else:\n",
    "                        sibling = sibling.next_sibling\n",
    "            if(episode=='Simpsons_Christmas_Stories'):\n",
    "                parent = soup.find('span', {'id':'Full_Story'}).parent\n",
    "                sibling = parent.next_sibling.next_sibling\n",
    "                syn += ' ' + str(sibling)\n",
    "                sibling = sibling.next_sibling.next_sibling.next_sibling\n",
    "                while(sibling.name == 'p' or sibling.name is None or sibling.name =='a' or sibling.name=='i'):\n",
    "                    if(sibling.name == 'p' or sibling.name=='a' or sibling.name=='i'):\n",
    "                        syn += ' ' + str(sibling.text)\n",
    "                        sibling = sibling.next_sibling\n",
    "                    else:\n",
    "                        sibling = sibling.next_sibling\n",
    "            if(episode=='How_the_Test_Was_Won'):\n",
    "                parent = soup.find('span', {'id':'Full_Story'}).parent\n",
    "                sibling = parent.next_sibling.next_sibling.next_sibling\n",
    "                syn += ' ' + str(sibling)\n",
    "                sibling = sibling.next_sibling\n",
    "                while(sibling.name == 'p' or sibling.name is None or sibling.name =='a' or sibling.name=='i'):\n",
    "                    if(sibling.name == 'p' or sibling.name=='a' or sibling.name=='i'):\n",
    "                        syn += ' ' + str(sibling.text)\n",
    "                        sibling = sibling.next_sibling\n",
    "                    else:\n",
    "                        sibling = sibling.next_sibling\n",
    "            else:\n",
    "                parent = soup.find('span', {'id':'Full_Story'}).parent\n",
    "                sibling = parent.next_sibling\n",
    "\n",
    "                while(sibling.name == 'p' or sibling.name is None or sibling.name =='a' or sibling.name=='i'):\n",
    "\n",
    "                    if(sibling.name == 'p' or sibling.name=='a' or sibling.name=='i'):\n",
    "                        syn += ' ' + str(sibling.text)\n",
    "                        sibling = sibling.next_sibling\n",
    "                    else:\n",
    "                        sibling = sibling.next_sibling\n",
    "\n",
    "        elif(soup.find('span', {'id':'The_Story'})):\n",
    "            \n",
    "            parent = soup.find('span', {'id':'The_Story'}).parent\n",
    "            sibling = parent.next_sibling\n",
    "            while(sibling.name == 'p' or sibling.name is None or sibling.name =='a' or sibling.name=='i'):\n",
    "                if(sibling.name == 'p' or sibling.name=='a'):\n",
    "                    syn += ' ' + str(sibling.text)\n",
    "                    sibling = sibling.next_sibling\n",
    "                else:\n",
    "                    sibling = sibling.next_sibling\n",
    "                    \n",
    "        elif(soup.find('span', {'id':'Full_story'})):\n",
    "            \n",
    "            parent = soup.find('span', {'id':'Full_story'}).parent\n",
    "            sibling = parent.next_sibling\n",
    "            while(sibling.name == 'p' or sibling.name is None or sibling.name =='a' or sibling.name=='i'):\n",
    "                if(sibling.name == 'p' or sibling.name=='a' or sibling.name=='i'):\n",
    "                    syn += ' ' + str(sibling.text)\n",
    "                    sibling = sibling.next_sibling\n",
    "                else:\n",
    "                    sibling = sibling.next_sibling\n",
    "\n",
    "        elif(soup.find('span', {'id':'Plot'})):\n",
    "            \n",
    "            parent = soup.find('span', {'id':'Plot'}).parent\n",
    "            sibling = parent.next_sibling\n",
    "            while(sibling.name == 'p' or sibling.name is None or sibling.name=='a' or sibling.name=='i'):\n",
    "                if(sibling.name == 'p' or sibling.name=='a' or sibling.name=='i'):\n",
    "                    syn += ' ' + str(sibling.text)\n",
    "                    sibling = sibling.next_sibling\n",
    "                else:\n",
    "                    sibling = sibling.next_sibling\n",
    "                    \n",
    "        elif(soup.find('span', {'id':'Full_Story.C2.A0'})):\n",
    "            \n",
    "            parent = soup.find('span', {'id':'Full_Story.C2.A0'}).parent\n",
    "            sibling = parent.next_sibling\n",
    "\n",
    "            while(sibling.name == 'p' or sibling.name is None or sibling.name =='a' or sibling.name=='i'):\n",
    "                if(sibling.name == 'p' or sibling.name=='a' or sibling.name=='i'):\n",
    "                    syn += ' ' + str(sibling.text)\n",
    "                    sibling = sibling.next_sibling\n",
    "                else:\n",
    "                    sibling = sibling.next_sibling\n",
    "\n",
    "                    \n",
    "    complete_name = os.path.join(os.path.expanduser('~'),save_path+'season'+str(season)+'/',str(ep_number)+'.txt')\n",
    "\n",
    "    with io.open(complete_name, \"w\", encoding=\"utf-8\") as f: \n",
    "        f.write(str(syn))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is then necessary to copy episode 608 and create episode 609, as these two episodes are the same episode - often referred to as episode \"608609\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.getcwd()+'/synopsis/season28/'\n",
    "path = save_path+\"608.txt\"\n",
    "text = io.open(path,'r',encoding='utf-8').read()\n",
    "with io.open(save_path+'609.txt', \"w\", encoding=\"utf-8\") as f: \n",
    "    f.write(str(text))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for ep in df_episodes_ratings['no_overall']:\n",
    "    season = df_episodes_ratings[df_episodes_ratings['no_overall']==ep]['season'].values[0]\n",
    "    with io.open(\"synopsis/season{}/{}.txt\".format(season,ep),'r', encoding='utf-8') as f:\n",
    "        desc = f.read()\n",
    "        desc = desc.lower()\n",
    "        rows.append([ep,desc])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc = pd.DataFrame(rows, columns = ['no_overall','desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>643</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>646</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>660</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall desc\n",
       "642         643     \n",
       "645         646     \n",
       "659         660     "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_desc[df_desc['desc']==' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping for a small synopsis of each episode\n",
    "\n",
    "As the descriptions for some episode might be quite long and cumbersome, we decided that obtaining a short *synopsis*, which is also represented on the same wiki website, could potentially be a better textual representation of each episode. We suspect that these textual representations of the epsiodes will differ quite a bit. The following scraping will be carried out in a very similar manner to what has been done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('short_synopsis'):\n",
    "        os.mkdir('short_synopsis')\n",
    "\n",
    "synopsis = []\n",
    "\n",
    "for season in df_episodes_ratings['season'].unique():\n",
    "    if not os.path.exists('short_synopsis/'+'season'+str(season)):\n",
    "        os.mkdir('short_synopsis/'+'season'+str(season))\n",
    "\n",
    "save_path = os.getcwd()+'/short_synopsis/'\n",
    "for episode in df_episodes_ratings['wiki_link']:\n",
    "    res = requests.get(\"https://simpsons.fandom.com/wiki/{}\".format(episode)).text\n",
    "    ep_number = df_episodes_ratings[df_episodes_ratings['wiki_link']==episode]['no_overall'].values[0]\n",
    "    season = int(df_episodes_ratings[df_episodes_ratings['wiki_link']==episode]['season'].values[0])\n",
    "    soup = BeautifulSoup(res,'html')\n",
    "    if(episode == 'The_Old_Man_and_the_Key'): # An edge case\n",
    "        syn=soup.find_all('p')[1].text\n",
    "    elif(soup.find('span', {'id':'Synopsis'})):\n",
    "        firstp = soup.find('span', {'id':'Synopsis'}).parent.findNext('p')\n",
    "        syn = firstp.text\n",
    "        while(firstp.nextSibling.name=='p'):\n",
    "            syn = str(syn) + ' ' + firstp.nextSibling.text\n",
    "            firstp = firstp.nextSibling\n",
    "    elif(soup.find('span', {'id':'Synopsis.'})):\n",
    "        syn = soup.find('span', {'id':'Synopsis.'}).parent.findNext('p').text\n",
    "    elif(soup.find('span', {'id':'Summary'})):\n",
    "        syn = soup.find('span', {'id':'Summary'}).parent.findNext('p').text\n",
    "    complete_name = os.path.join(os.path.expanduser('~'),save_path+'season'+str(season)+'/',str(ep_number)+'.txt')\n",
    "    with io.open(complete_name, \"w\", encoding=\"utf-8\") as f: \n",
    "        f.write(str(syn))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.getcwd()+'/short_synopsis/season28/'\n",
    "path = save_path+\"608.txt\"\n",
    "text = io.open(path,'r',encoding='utf-8').read()\n",
    "with io.open(save_path+'609.txt', \"w\", encoding=\"utf-8\") as f: \n",
    "    f.write(str(text))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for ep in df_episodes_ratings['no_overall']:\n",
    "    season = df_episodes_ratings[df_episodes_ratings['no_overall']==ep]['season'].values[0]\n",
    "    with io.open(\"short_synopsis/season{}/{}.txt\".format(season,ep),'r', encoding='utf-8') as f:\n",
    "        desc = f.read()\n",
    "        desc = desc.lower()\n",
    "        rows.append([ep,desc])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short_desc = pd.DataFrame(rows, columns = ['no_overall','short_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>short_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>131</td>\n",
       "      <td>after a series of misadventures, the simpson c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>446</td>\n",
       "      <td>marge poses as a calendar model for charity bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>304</td>\n",
       "      <td>while at a cotillion after observing an annual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>415</td>\n",
       "      <td>lisa gets recruited for chazz busby's ballet a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>205</td>\n",
       "      <td>when homer realizes, at 38.1 years of age, tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall                                         short_desc\n",
       "130         131  after a series of misadventures, the simpson c...\n",
       "445         446  marge poses as a calendar model for charity bu...\n",
       "303         304  while at a cotillion after observing an annual...\n",
       "414         415  lisa gets recruited for chazz busby's ballet a...\n",
       "204         205  when homer realizes, at 38.1 years of age, tha..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_short_desc.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>short_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [no_overall, short_desc]\n",
       "Index: []"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_short_desc[df_short_desc['short_desc']==' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_url = 'https://simpsons.fandom.com/wiki/Portal:All_Simpson_Characters'\n",
    "res = requests.get(char_url).text\n",
    "soup = BeautifulSoup(res,'html')\n",
    "characters = []\n",
    "for item in soup.find_all('div', class_='wikia-gallery-item'):\n",
    "    wiki_link_container = item.find('div', class_='lightbox-caption')\n",
    "    if(wiki_link_container.find('a',href=True)):\n",
    "        wiki_link = wiki_link_container.find('a',href=True)['href']\n",
    "        characters.append([item.text,wiki_link.replace('/wiki/','')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char = pd.DataFrame(characters)\n",
    "df_char.columns = ['name', 'char_wiki_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char['extra_wiki_link'] = df_char['char_wiki_link'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that must be noted is that some characters will not be referred to by their first name or even last name. For example, Abraham Simpson II will more often than not be referred to as *Grampa* or *Abe*. The same can be said about Charles Montgomery Burns will most likely be referred to as *Mr. Burns* or *Burns*. \n",
    "\n",
    "The character of Robert Terwilliger will most likely be referred to as *Sideshow Bob*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.at[5,'name'] = 'Abe'\n",
    "df_char.at[7,'name'] = 'Mr. Burns'\n",
    "\n",
    "df_char.at[df_char.index[df_char['name'] == 'Robert Terwilliger'].tolist(), 'name'] = 'Sideshow Bob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_names = df_char['name'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = []\n",
    "for i,names in enumerate(first_names):\n",
    "    rid = False\n",
    "    for name in names:\n",
    "        if('\\'s' in name or 's\\'' in name):\n",
    "            rid = True\n",
    "            break\n",
    "        elif('the' in name):\n",
    "            rid = False\n",
    "        elif(name[0].islower()):\n",
    "            if(name != 'bin' and name != 'von'):\n",
    "                rid = True\n",
    "                break\n",
    "        elif(name.isdigit()):\n",
    "            rid = True\n",
    "            break\n",
    "    if(rid):\n",
    "        fname = ''\n",
    "        for name in names:\n",
    "            fname = fname + ' ' + name\n",
    "        fnames.append(fname[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char = df_char[~df_char['name'].isin(fnames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char['name'] = df_char['name'].apply(lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x))\n",
    "df_char['name'] = df_char['name'].apply(lambda x: re.sub(' +', ' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char['name'] = df_char['name'].apply(lambda x: x.replace(\"\\\"\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characters that are irrelevant\n",
    "\n",
    "rid_of_chars = [18, 120, 138, 255, 254, 253, 260, 276, 281, 287, 288, 295, 315, 482, 487,\n",
    "                498, 501, 502, 504, 505, 517, 526, 529, 533, 542,543, 547, 557, 559, 561, 562, \n",
    "                599, 600, 601, 602, 603, 605, 606, 610, 611, 612, 613, 614, 615, 616, 617, 618, \n",
    "                619, 620, 621, 622, 607, 608, 579, 524, 604, 320, 593]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.drop(rid_of_chars,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char['name_split'] = df_char['name'].apply(lambda x: str(x).split(' '))\n",
    "df_char['num_names'] = df_char['name_split'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>char_wiki_link</th>\n",
       "      <th>extra_wiki_link</th>\n",
       "      <th>name_split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_names</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>371</td>\n",
       "      <td>371</td>\n",
       "      <td>371</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  char_wiki_link  extra_wiki_link  name_split\n",
       "num_names                                                   \n",
       "1            92              92               92          92\n",
       "2           371             371              371         371\n",
       "3            94              94               94          94\n",
       "4             9               9                9           9"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_char.groupby('num_names').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "irr_names = ['State','Black','Blue-Haired','Lawyer','Cowboy','Man','City','Capital','Database',\n",
    "            'Boy','Little','Fat','Frog','God','Handsome','Judge','Guy','Book','Old','Man','Number','One',\n",
    "            'Man','Rich','Veterinarian','Poor','Thief','Relative','Manager','Millionaire','Actor','Old',\n",
    "            'Lady','New','Stop','Rest','Three','Unnamed','Baby','Blow','Horn','E-mail','Kid','White','Chew',\n",
    "            'Strangles','Talking','Dog','Taquito','Gay','Colonel','Mathemagician','Fish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_remove = []\n",
    "for name_split in df_char['name_split']:\n",
    "    for name in name_split:\n",
    "        if(len(name)>2 and '.' not in name and name!='The' and name!='the' and name not in irr_names):\n",
    "            names_to_remove.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_remove = list(set(names_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Scripts for the episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: red;\">simpson csv file</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_script = pd.read_csv('the-simpsons-by-the-data-QueryResult.csv', low_memory = False)\n",
    "df_script_episodes = pd.read_csv('the-simpsons-by-the-data-QueryResult_episodes.csv')\n",
    "df_script_episodes.columns = ['episode_id','episode_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>episode_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>598</td>\n",
       "      <td>Friends and Family\"[203]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>599</td>\n",
       "      <td>The Town\"[205]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>600</td>\n",
       "      <td>Treehouse of Horror XXVII\"[207]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode_id                     episode_name\n",
       "323         598         Friends and Family\"[203]\n",
       "324         599                   The Town\"[205]\n",
       "325         600  Treehouse of Horror XXVII\"[207]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_script_episodes[df_script_episodes['episode_name'].str[-1] == ']']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_script_episodes.at[323,'episode_name'] = 'Friends and Family'\n",
    "df_script_episodes.at[324,'episode_name'] = 'The Town'\n",
    "df_script_episodes.at[325,'episode_name'] = 'Treehouse of Horror XXVII'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_script['episode_id'].nunique() == df_script['episode_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episode_attributes = df_episodes_ratings[['episode_name','no_overall','season','no_in_season']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_script = pd.merge(df_script,df_script_episodes,on='episode_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>episode_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148761</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148762</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "      <td>false</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Car</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148763</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>true</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "      <td>ooo careful homer</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        episode_id  number                                      raw_text  \\\n",
       "148761           1       0  (Street: ext. street - establishing - night)   \n",
       "148762           1       1                       (Car: int. car - night)   \n",
       "148763           1       2           Marge Simpson: Ooo, careful, Homer.   \n",
       "\n",
       "       speaking_line raw_character_text raw_location_text  \\\n",
       "148761         false                NaN            Street   \n",
       "148762         false                NaN               Car   \n",
       "148763          true      Marge Simpson               Car   \n",
       "\n",
       "                spoken_words    normalized_text  \\\n",
       "148761                   NaN                NaN   \n",
       "148762                   NaN                NaN   \n",
       "148763  Ooo, careful, Homer.  ooo careful homer   \n",
       "\n",
       "                             episode_name  \n",
       "148761  Simpsons Roasting on an Open Fire  \n",
       "148762  Simpsons Roasting on an Open Fire  \n",
       "148763  Simpsons Roasting on an Open Fire  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_script.sort_values(['episode_id','number']).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_script = pd.merge(df_script,df_episode_attributes,on='episode_name',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The PTA Disbands', 'Alone Again, Natura-diddily',\n",
       "       'Bye, Bye, Nerdie', \"I'm Spelling As Fast As I Can\",\n",
       "       'A Star Is Born-Again', \"Thank God It's Doomsday\",\n",
       "       'Million-Dollar Abie', \"Dial 'N' for Nerder\",\n",
       "       'How Munched is That Birdie in the Window?', \"Ned 'n Edna's Blend\",\n",
       "       'Love is a Many-Splintered Thing'], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_script[df_script['season'].isna()]['episode_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>wiki_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>124</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>The PTA Disbands!</td>\n",
       "      <td>11.80</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1995</td>\n",
       "      <td>04</td>\n",
       "      <td>16</td>\n",
       "      <td>The_PTA_Disbands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>240</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>Alone Again, Natura-Diddily</td>\n",
       "      <td>10.80</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2000</td>\n",
       "      <td>02</td>\n",
       "      <td>13</td>\n",
       "      <td>Alone_Again,_Natura-Diddily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>264</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>Bye Bye Nerdie</td>\n",
       "      <td>16.10</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2001</td>\n",
       "      <td>03</td>\n",
       "      <td>11</td>\n",
       "      <td>Bye_Bye_Nerdie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>303</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>I'm Spelling as Fast as I Can</td>\n",
       "      <td>22.10</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2003</td>\n",
       "      <td>02</td>\n",
       "      <td>16</td>\n",
       "      <td>I%27m_Spelling_as_Fast_as_I_Can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>304</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>A Star Is Born Again</td>\n",
       "      <td>14.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2003</td>\n",
       "      <td>03</td>\n",
       "      <td>02</td>\n",
       "      <td>A_Star_is_Born-Again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>354</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>Thank God, It's Doomsday</td>\n",
       "      <td>10.05</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2005</td>\n",
       "      <td>05</td>\n",
       "      <td>08</td>\n",
       "      <td>Thank_God_It%27s_Doomsday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>372</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>Million Dollar Abie</td>\n",
       "      <td>7.83</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2006</td>\n",
       "      <td>04</td>\n",
       "      <td>02</td>\n",
       "      <td>Million-Dollar_Abie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>414</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>Dial \"N\" for Nerder</td>\n",
       "      <td>7.30</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2008</td>\n",
       "      <td>03</td>\n",
       "      <td>09</td>\n",
       "      <td>Dial_%22N%22_for_Nerder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>471</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>How Munched Is That Birdie in the Window?</td>\n",
       "      <td>9.38</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2010</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>How_Munched_is_That_Birdie_in_the_Window%3F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>507</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>Ned 'n' Edna's Blend Agenda</td>\n",
       "      <td>4.07</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2012</td>\n",
       "      <td>05</td>\n",
       "      <td>13</td>\n",
       "      <td>Ned_%27N_Edna%27s_Blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>520</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>Love Is a Many-Splintered Thing</td>\n",
       "      <td>4.19</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2013</td>\n",
       "      <td>02</td>\n",
       "      <td>10</td>\n",
       "      <td>Love_is_a_Many-Splintered_Thing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall  season  no_in_season  \\\n",
       "123         124       6            21   \n",
       "239         240      11            14   \n",
       "263         264      12            16   \n",
       "302         303      14            12   \n",
       "303         304      14            13   \n",
       "353         354      16            19   \n",
       "371         372      17            16   \n",
       "413         414      19            14   \n",
       "470         471      22             7   \n",
       "506         507      23            21   \n",
       "519         520      24            12   \n",
       "\n",
       "                                  episode_name  viewers (millions)  \\\n",
       "123                          The PTA Disbands!               11.80   \n",
       "239                Alone Again, Natura-Diddily               10.80   \n",
       "263                             Bye Bye Nerdie               16.10   \n",
       "302              I'm Spelling as Fast as I Can               22.10   \n",
       "303                       A Star Is Born Again               14.40   \n",
       "353                   Thank God, It's Doomsday               10.05   \n",
       "371                        Million Dollar Abie                7.83   \n",
       "413                        Dial \"N\" for Nerder                7.30   \n",
       "470  How Munched Is That Birdie in the Window?                9.38   \n",
       "506                Ned 'n' Edna's Blend Agenda                4.07   \n",
       "519            Love Is a Many-Splintered Thing                4.19   \n",
       "\n",
       "     IMDB rating  year month day                                    wiki_link  \n",
       "123          8.1  1995    04  16                             The_PTA_Disbands  \n",
       "239          7.7  2000    02  13                  Alone_Again,_Natura-Diddily  \n",
       "263          6.6  2001    03  11                               Bye_Bye_Nerdie  \n",
       "302          7.2  2003    02  16              I%27m_Spelling_as_Fast_as_I_Can  \n",
       "303          6.9  2003    03  02                         A_Star_is_Born-Again  \n",
       "353          7.2  2005    05  08                    Thank_God_It%27s_Doomsday  \n",
       "371          6.3  2006    04  02                          Million-Dollar_Abie  \n",
       "413          7.3  2008    03  09                      Dial_%22N%22_for_Nerder  \n",
       "470          6.2  2010    11  28  How_Munched_is_That_Birdie_in_the_Window%3F  \n",
       "506          6.5  2012    05  13                      Ned_%27N_Edna%27s_Blend  \n",
       "519          6.1  2013    02  10              Love_is_a_Many-Splintered_Thing  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings[(df_episodes_ratings['episode_name'].str.contains('Nerder')) | \n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Natura-')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Disbands')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Nerdie')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Star Is Born')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Dollar Abie')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Blend')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Splintered')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Munched')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Doomsday')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Spelling'))]                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_names = [name for name in df_episodes_ratings[(df_episodes_ratings['episode_name'].str.contains('Nerder')) | \n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Natura-')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Nerdie')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Disbands')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Star Is Born')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Dollar Abie')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Blend')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Splintered')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Munched')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Doomsday')) |\n",
    "                    (df_episodes_ratings['episode_name'].str.contains('Spelling'))]['episode_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The PTA Disbands!',\n",
       " 'Alone Again, Natura-Diddily',\n",
       " 'Bye Bye Nerdie',\n",
       " \"I'm Spelling as Fast as I Can\",\n",
       " 'A Star Is Born Again',\n",
       " \"Thank God, It's Doomsday\",\n",
       " 'Million Dollar Abie',\n",
       " 'Dial \"N\" for Nerder',\n",
       " 'How Munched Is That Birdie in the Window?',\n",
       " \"Ned 'n' Edna's Blend Agenda\",\n",
       " 'Love Is a Many-Splintered Thing']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_names = [name for name in df_script[df_script['season'].isna()]['episode_name'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The PTA Disbands',\n",
       " 'Alone Again, Natura-diddily',\n",
       " 'Bye, Bye, Nerdie',\n",
       " \"I'm Spelling As Fast As I Can\",\n",
       " 'A Star Is Born-Again',\n",
       " \"Thank God It's Doomsday\",\n",
       " 'Million-Dollar Abie',\n",
       " \"Dial 'N' for Nerder\",\n",
       " 'How Munched is That Birdie in the Window?',\n",
       " \"Ned 'n Edna's Blend\",\n",
       " 'Love is a Many-Splintered Thing']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incorrect_names = df_script_episodes[df_script_episodes['episode_name'].isin(incorrect_names)].sort_values('episode_id')\n",
    "df_incorrect_names_indices = [index for index in df_incorrect_names.reset_index()['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,name in enumerate(correct_names):\n",
    "    df_script_episodes.at[df_incorrect_names_indices[i], 'episode_name'] = correct_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_script.drop(['season','no_overall','no_in_season','episode_name'],axis=1,inplace=True)\n",
    "df_script = pd.merge(df_script,df_script_episodes,on='episode_id',how='left')\n",
    "df_script = pd.merge(df_script,df_episode_attributes,on='episode_name',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [episode_id, number, raw_text, speaking_line, raw_character_text, raw_location_text, spoken_words, normalized_text, no_overall, season, no_in_season]\n",
       "Index: []"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_script[df_script['season'].isnull()].groupby('episode_name').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_script.sort_values(by=['episode_id','number']).to_csv('scripts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scripts = pd.read_csv('scripts.csv').drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ooo careful homer'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scripts[~df_scripts['normalized_text'].isna()].reset_index(drop=True).normalized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_scripts = {}\n",
    "for i in range(1,569):\n",
    "    episode_scripts[i] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in df_scripts[~df_scripts['normalized_text'].isna()].reset_index(drop=True).iterrows():\n",
    "    episode_scripts[row.no_overall] = episode_scripts[row.no_overall] + ' ' + row.normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episode_scripts = pd.DataFrame.from_dict(episode_scripts, orient='index',columns=['script']).reset_index()\n",
    "df_episode_scripts.columns = ['no_overall', 'script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [no_overall, script]\n",
       "Index: []"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episode_scripts[df_episode_scripts['script'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Natural Language Processing<a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "In order to represent the text data better, some useful natural language processing methods will be deployed. \n",
    "\n",
    "First, all *stopwords*, along with the words \"*episode*\" and \"*begins*\" will be removed from the descriptions as they do not pertain any information about the episode itself, and appear quite often. \n",
    "\n",
    "Then, the description is split into *tokens* (words) and lastly we use stemming. For this particular objective, the Snowball (Porter2) stemmer will be used, as it is widely regarded as an improvement of the gentle Porter stemmer and is computationally faster [[REF]](https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg) Then the words can then be analyzed and very infrequent words will be removed, as will very frequent words. \n",
    "\n",
    "Character names will also appear quite often in the description of each epsiode. Here, we have chosen to remove them from the description. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop wards will need to be created and appended with some special words that appear more often than they should, e.g. *episode* and *begins*, as this is a description of an episode on a Wiki page. Also, some character names will be added to the stopwords in order to remove them from the description later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['episode','begins']\n",
    "STOP_WORDS_EN = stopwords.words('english')\n",
    "for word in STOP_WORDS_EN:\n",
    "    stop_words.append(re.sub(r'[^\\w]', ' ', word).replace(' ',''))\n",
    "\n",
    "for word in ['episode','begins','homer','marge','bart','maggie','lisa','simpson','simpsons','burns',\n",
    "             'milhouse','krusty','moe','skinner','wiggum','ned','flanders','simon','smithers','yeah','yes','no','okay','hehe','heh']:\n",
    "    stop_words.append(word.lower())\n",
    "for name in names_to_remove:\n",
    "    stop_words.append(name.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "\n",
    "for ep in df_episodes_ratings['no_overall']:\n",
    "    season = df_episodes_ratings[df_episodes_ratings['no_overall']==ep]['season'].values[0]\n",
    "    with io.open(\"synopsis/season{}/{}.txt\".format(season,ep),'r', encoding='utf-8') as f:\n",
    "        desc = f.read()\n",
    "        desc = re.sub(r'[^\\w]', ' ', desc)\n",
    "        desc = re.sub(\" \\d+\", \" \", desc) #removing numbers\n",
    "        desc = desc.replace(r' +',' ').replace('\\n','')\n",
    "        desc = desc.split(' ')\n",
    "        desc = [word.lower() for word in desc if word.lower() not in stop_words]\n",
    "        desc = [stemmer.stem(word.lower()) for word in desc if len(word)>2]\n",
    "        rows.append([ep, desc])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc = pd.DataFrame(rows, columns = ['no_overall','desc'])\n",
    "df_desc = df_desc[:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a column is created in order to display the length of each description for further analysis and processing of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc['desc_len'] = df_desc['desc'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>desc</th>\n",
       "      <th>desc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [no_overall, desc, desc_len]\n",
       "Index: []"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_desc[df_desc['desc_len']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_episodes_ratings.merge(df_desc, on='no_overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>wiki_link</th>\n",
       "      <th>desc</th>\n",
       "      <th>desc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [no_overall, season, no_in_season, episode_name, viewers (millions), IMDB rating, year, month, day, wiki_link, desc, desc_len]\n",
       "Index: []"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['desc_len']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "\n",
    "for ep in df_episodes_ratings['no_overall']:\n",
    "    season = df_episodes_ratings[df_episodes_ratings['no_overall']==ep]['season'].values[0]\n",
    "    with io.open(\"short_synopsis/season{}/{}.txt\".format(season,ep),'r', encoding='utf-8') as f:\n",
    "        desc = f.read()\n",
    "        desc = re.sub(r'[^\\w]', ' ', desc)\n",
    "        desc = re.sub(\" \\d+\", \" \", desc) #removing numbers\n",
    "        desc = desc.replace(r' +',' ').replace('\\n','')\n",
    "        desc = desc.split(' ')\n",
    "        trun_desc = desc\n",
    "        desc = [word.lower() for word in desc if word.lower() not in stop_words]\n",
    "        desc = [stemmer.stem(word.lower()) for word in desc if len(word)>2]\n",
    "\n",
    "        rows.append([ep, desc])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short_desc = pd.DataFrame(rows, columns = ['no_overall','short_desc'])\n",
    "df_short_desc = df_short_desc[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short_desc['short_desc_len'] = df_short_desc['short_desc'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_short_desc, on='no_overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['no_overall', 'season', 'no_in_season', 'episode_name',\n",
       "       'viewers (millions)', 'IMDB rating', 'year', 'month', 'day',\n",
       "       'wiki_link', 'desc', 'desc_len', 'short_desc', 'short_desc_len'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['no_overall', 'season', 'no_in_season', 'episode_name',\n",
    "       'mil_viewers', 'rating', 'year', 'month', 'day',\n",
    "       'wiki_link', 'desc', 'desc_len','short_desc','short_desc_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('wiki_link',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mil_viewers'] = df['mil_viewers'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = df['day'].replace('22[a]','22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_overall'] = df['no_overall'].apply(lambda x: int(x))\n",
    "df['no_in_season'] = df['no_in_season'].apply(lambda x: int(x))\n",
    "df['season'] = df['season'].apply(lambda x: int(x))\n",
    "df['year'] = df['year'].apply(lambda x: int(x))\n",
    "df['month'] = df['month'].apply(lambda x: int(x))\n",
    "df['day'] = df['day'].apply(lambda x: int(x))\n",
    "df['rating'] = df['rating'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set now looks like this, where the column *desc* is essentially a bag of words for each episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>mil_viewers</th>\n",
       "      <th>rating</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>desc</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>short_desc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>442</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer the Whopper</td>\n",
       "      <td>8.31</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>[convinc, book, guy, publish, book, wrote, tit...</td>\n",
       "      <td>84</td>\n",
       "      <td>[book, guy, book, hero, everyman, disguis, bec...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>539</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>Steal This Episode</td>\n",
       "      <td>12.04</td>\n",
       "      <td>7.6</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[keep, hear, spoiler, recent, releas, movi, ma...</td>\n",
       "      <td>377</td>\n",
       "      <td>[get, kick, church, theater, learn, pirat, mov...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>156</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>The Homer They Fall</td>\n",
       "      <td>17.00</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1996</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>[book, guy, elementari, tavern, save, genet, d...</td>\n",
       "      <td>20</td>\n",
       "      <td>[buy, extravag, belt, trip, mall, school, bull...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>454</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>The Color Yellow</td>\n",
       "      <td>6.08</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>[attempt, remov, stump, blow, stump, land, kia...</td>\n",
       "      <td>297</td>\n",
       "      <td>[research, famili, someon, total, boob, crimin...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>245</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>Kill the Alligator and Run</td>\n",
       "      <td>7.46</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2000</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>[get, magazin, mail, load, person, test, relen...</td>\n",
       "      <td>216</td>\n",
       "      <td>[suffer, insomnia, due, stress, age, attempt, ...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>327</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>The Ziff Who Came to Dinner</td>\n",
       "      <td>10.70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>[take, movi, bring, boy, taken, senior, citize...</td>\n",
       "      <td>265</td>\n",
       "      <td>[old, prom, date, return, lost, money, interne...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>Bart Gets an Elephant</td>\n",
       "      <td>17.00</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1994</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>[clean, day, hous, win, kbbl, radio, contest, ...</td>\n",
       "      <td>242</td>\n",
       "      <td>[win, radio, contest, choos, two, prize, ten, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>393</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>Rome-Old and Juli-Eh</td>\n",
       "      <td>8.98</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2007</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>[supris, famili, newli, decor, basement, recre...</td>\n",
       "      <td>324</td>\n",
       "      <td>[declar, bankruptci, impress, would, protect, ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>Lisa's Substitute</td>\n",
       "      <td>17.70</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1991</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>[teacher, think, come, lyme, diseas, replac, s...</td>\n",
       "      <td>148</td>\n",
       "      <td>[teacher, get, lyme, diseas, bergstrom, take, ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Bart the Murderer</td>\n",
       "      <td>20.80</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1991</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>[day, prepar, school, day, suppos, field, trip...</td>\n",
       "      <td>213</td>\n",
       "      <td>[lousi, day, school, accident, stumbl, legitim...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall  season  no_in_season                 episode_name  \\\n",
       "441         442      21             1            Homer the Whopper   \n",
       "538         539      25             9           Steal This Episode   \n",
       "155         156       8             3          The Homer They Fall   \n",
       "453         454      21            13             The Color Yellow   \n",
       "244         245      11            19   Kill the Alligator and Run   \n",
       "326         327      15            14  The Ziff Who Came to Dinner   \n",
       "97           98       5            17        Bart Gets an Elephant   \n",
       "392         393      18            15         Rome-Old and Juli-Eh   \n",
       "31           32       2            19            Lisa's Substitute   \n",
       "38           39       3             4            Bart the Murderer   \n",
       "\n",
       "     mil_viewers  rating  year  month  day  \\\n",
       "441         8.31     7.0  2009      9   27   \n",
       "538        12.04     7.6  2014      1    5   \n",
       "155        17.00     8.1  1996     11   10   \n",
       "453         6.08     6.3  2010      2   21   \n",
       "244         7.46     6.5  2000      4   30   \n",
       "326        10.70     6.7  2004      3   14   \n",
       "97         17.00     7.9  1994      3   31   \n",
       "392         8.98     6.1  2007      3   11   \n",
       "31         17.70     8.5  1991      4   25   \n",
       "38         20.80     8.6  1991     10   10   \n",
       "\n",
       "                                                  desc  desc_len  \\\n",
       "441  [convinc, book, guy, publish, book, wrote, tit...        84   \n",
       "538  [keep, hear, spoiler, recent, releas, movi, ma...       377   \n",
       "155  [book, guy, elementari, tavern, save, genet, d...        20   \n",
       "453  [attempt, remov, stump, blow, stump, land, kia...       297   \n",
       "244  [get, magazin, mail, load, person, test, relen...       216   \n",
       "326  [take, movi, bring, boy, taken, senior, citize...       265   \n",
       "97   [clean, day, hous, win, kbbl, radio, contest, ...       242   \n",
       "392  [supris, famili, newli, decor, basement, recre...       324   \n",
       "31   [teacher, think, come, lyme, diseas, replac, s...       148   \n",
       "38   [day, prepar, school, day, suppos, field, trip...       213   \n",
       "\n",
       "                                            short_desc  short_desc_len  \n",
       "441  [book, guy, book, hero, everyman, disguis, bec...              47  \n",
       "538  [get, kick, church, theater, learn, pirat, mov...              17  \n",
       "155  [buy, extravag, belt, trip, mall, school, bull...              31  \n",
       "453  [research, famili, someon, total, boob, crimin...              22  \n",
       "244  [suffer, insomnia, due, stress, age, attempt, ...              27  \n",
       "326  [old, prom, date, return, lost, money, interne...              21  \n",
       "97   [win, radio, contest, choos, two, prize, ten, ...              20  \n",
       "392  [declar, bankruptci, impress, would, protect, ...              33  \n",
       "31   [teacher, get, lyme, diseas, bergstrom, take, ...              32  \n",
       "38   [lousi, day, school, accident, stumbl, legitim...              26  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['script'] = df_episode_scripts['script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['script'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['script'] = df['script'].apply(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>mil_viewers</th>\n",
       "      <th>rating</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>desc</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>short_desc_len</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>182</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>Treehouse of Horror VIII</td>\n",
       "      <td>10.90</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1997</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>[fox, censor, simpli, name, fox, censor, sit, ...</td>\n",
       "      <td>472</td>\n",
       "      <td>[fox, censor, open, show]</td>\n",
       "      <td>4</td>\n",
       "      <td>and thats how an heroic hippo became a deputy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>233</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>Eight Misbehavin'</td>\n",
       "      <td>9.20</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1999</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>[famili, visit, shøp, strang, enough, spell, d...</td>\n",
       "      <td>135</td>\n",
       "      <td>[wife, give, birth]</td>\n",
       "      <td>3</td>\n",
       "      <td>these swedish furniture designers sure have so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>265</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>Simpson Safari</td>\n",
       "      <td>13.30</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[hospit, emerg, room, swallow, issu, time, mag...</td>\n",
       "      <td>372</td>\n",
       "      <td>[kiteng, introduc, famili]</td>\n",
       "      <td>3</td>\n",
       "      <td>olive oil asparagus if your mother wasnt so fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>336</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Treehouse of Horror XV</td>\n",
       "      <td>11.29</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2004</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>[parodi, perfect, stranger, two, alien, prepar...</td>\n",
       "      <td>313</td>\n",
       "      <td>[titl]</td>\n",
       "      <td>1</td>\n",
       "      <td>we now return to keepin it kodos starring kang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>424</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>Treehouse of Horror XIX</td>\n",
       "      <td>12.48</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>[open, scene, tri, vote, democrat, senat, bara...</td>\n",
       "      <td>540</td>\n",
       "      <td>[logo]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>435</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>Wedding for Disaster</td>\n",
       "      <td>6.58</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>[church, reverend, announc, parson, come, town...</td>\n",
       "      <td>304</td>\n",
       "      <td>[see, fight]</td>\n",
       "      <td>2</td>\n",
       "      <td>and so in summarythere are only two real comma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>459</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>Chief of Hearts</td>\n",
       "      <td>5.93</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>[walk, bank, eat, candi, appl, arriv, bank, ob...</td>\n",
       "      <td>191</td>\n",
       "      <td>[chief]</td>\n",
       "      <td>1</td>\n",
       "      <td>i cant believe youre making us go to a birthda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall  season  no_in_season              episode_name  mil_viewers  \\\n",
       "181         182       9             4  Treehouse of Horror VIII        10.90   \n",
       "232         233      11             7         Eight Misbehavin'         9.20   \n",
       "264         265      12            17            Simpson Safari        13.30   \n",
       "335         336      16             1    Treehouse of Horror XV        11.29   \n",
       "423         424      20             4   Treehouse of Horror XIX        12.48   \n",
       "434         435      20            15      Wedding for Disaster         6.58   \n",
       "458         459      21            18           Chief of Hearts         5.93   \n",
       "\n",
       "     rating  year  month  day  \\\n",
       "181     8.2  1997     10   26   \n",
       "232     7.0  1999     11   21   \n",
       "264     6.8  2001      4    1   \n",
       "335     7.4  2004     11    7   \n",
       "423     7.0  2008     11    2   \n",
       "434     6.6  2009      3   29   \n",
       "458     6.8  2010      4   18   \n",
       "\n",
       "                                                  desc  desc_len  \\\n",
       "181  [fox, censor, simpli, name, fox, censor, sit, ...       472   \n",
       "232  [famili, visit, shøp, strang, enough, spell, d...       135   \n",
       "264  [hospit, emerg, room, swallow, issu, time, mag...       372   \n",
       "335  [parodi, perfect, stranger, two, alien, prepar...       313   \n",
       "423  [open, scene, tri, vote, democrat, senat, bara...       540   \n",
       "434  [church, reverend, announc, parson, come, town...       304   \n",
       "458  [walk, bank, eat, candi, appl, arriv, bank, ob...       191   \n",
       "\n",
       "                     short_desc  short_desc_len  \\\n",
       "181   [fox, censor, open, show]               4   \n",
       "232         [wife, give, birth]               3   \n",
       "264  [kiteng, introduc, famili]               3   \n",
       "335                      [titl]               1   \n",
       "423                      [logo]               1   \n",
       "434                [see, fight]               2   \n",
       "458                     [chief]               1   \n",
       "\n",
       "                                                script  \n",
       "181  and thats how an heroic hippo became a deputy ...  \n",
       "232  these swedish furniture designers sure have so...  \n",
       "264  olive oil asparagus if your mother wasnt so fa...  \n",
       "335  we now return to keepin it kodos starring kang...  \n",
       "423                                                     \n",
       "434  and so in summarythere are only two real comma...  \n",
       "458  i cant believe youre making us go to a birthda...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['short_desc_len']<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_script(script):\n",
    "    script = re.sub(r'[^\\w]', ' ', script)\n",
    "    script = re.sub(\" \\d+\", \" \", script) #removing numbers\n",
    "    script = script.replace(r' +',' ').replace('\\n','')\n",
    "    script = script.split(' ')\n",
    "    script = [word.lower() for word in script if word.lower() not in stop_words]\n",
    "    script = [stemmer.stem(word.lower()) for word in script if len(word)>2]\n",
    "    return script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['script'] = df['script'].apply(clean_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyse the *dictionary* which will be used for the topic modelling, all words that appear over all descriptions of the episodes will be analyzed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_all_words = []\n",
    "for desc in df['desc']:\n",
    "    for word in desc:\n",
    "        desc_all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There is a total of {} unique words in the log description of all episodes\".format(len(set(desc_all_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 50 most common words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(desc_all_words).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the 50 most uncommon words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(desc_all_words).most_common()[:-50-1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common words that appear in the descriptions are words that one would expect to appear often in a description of any episode of The Simpsons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(list(Counter(desc_all_words).values()), rug=True)\n",
    "plt.title(\"The distribution of the ocurrance rate of words in the long description of an episode\",size=16)\n",
    "plt.xlabel(\"Occurance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(ticks=range(0,1500,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the short description, these are the most common and least common words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_all_words = []\n",
    "for desc in df['short_desc']:\n",
    "    for word in desc:\n",
    "        short_all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There is a total of {} unique words in the log description of all episodes\".format(len(set(short_all_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 50 most common words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(short_all_words).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the 50 most uncommon words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(short_all_words).most_common()[:-50-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(list(Counter(short_all_words).values()), rug=True)\n",
    "plt.title(\"The distribution of the ocurrance rate of words in a description of an episode\",size=16)\n",
    "plt.xlabel(\"Occurance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(ticks=range(0,250,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scripts, these are the occurance frequencies: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_all_words = []\n",
    "for script in df['script']:\n",
    "    for word in script:\n",
    "        script_all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There is a total of {} unique words in the scripts of all episodes\".format(len(set(script_all_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(script_all_words).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(script_all_words).most_common()[:-50-1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(list(Counter(script_all_words).values()), rug=True)\n",
    "plt.title(\"The distribution of the ocurrance rate of words in a script of an episode\",size=16)\n",
    "plt.xlabel(\"Occurance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the distribution of how often unique words occur in the long description and short description of episodes, words with a very high frequency and a very low frequency were removed from each episodes description.\n",
    "\n",
    "For the long description, words occuring more than 250 times or less than 20 times will be removed. \n",
    "\n",
    "For the short description, words occuring less than 5 times or more than 100 times will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_words_to_remove = list(dict(Counter(desc_all_words).most_common(20)).keys()) + list(dict(Counter(desc_all_words).most_common()[:-500:-1]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_desc_words_to_remove = list(dict(Counter(short_all_words).most_common(20)).keys()) + list(dict(Counter(short_all_words).most_common()[:-400:-1]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_words_to_remove = list(dict(Counter(script_all_words).most_common(20)).keys()) + list(dict(Counter(script_all_words).most_common()[:-5000:-1]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_high_lo_freq(original_words, words_to_remove):\n",
    "    for word in words_to_remove:\n",
    "        while word in original_words:\n",
    "            original_words.remove(word)\n",
    "    return original_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['desc'] = df['desc'].apply(lambda x: remove_high_lo_freq(x, desc_words_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['short_desc'] = df['short_desc'].apply(lambda x: remove_high_lo_freq(x, short_desc_words_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['script'] = df['script'].apply(lambda x: remove_high_lo_freq(x, script_words_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['desc_len'] = df['desc'].apply(lambda x: len(x))\n",
    "df['short_desc_len'] = df['short_desc'].apply(lambda x: len(x))\n",
    "df['script_len'] = df['script'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_all_words = []\n",
    "for desc in df['desc']:\n",
    "    for word in desc:\n",
    "        desc_all_words.append(word)\n",
    "        \n",
    "Counter(desc_all_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_all_words = []\n",
    "for desc in df['short_desc']:\n",
    "    for word in desc:\n",
    "        short_all_words.append(word)\n",
    "        \n",
    "Counter(short_all_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_all_words = []\n",
    "for script in df['script']:\n",
    "    for word in script:\n",
    "        script_all_words.append(word)\n",
    "        \n",
    "Counter(script_all_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['desc_string'] = df['desc'].apply(lambda x: ' '.join(x))\n",
    "df['short_desc_string'] = df['short_desc'].apply(lambda x: ' '.join(x))\n",
    "df['script_string'] = df['script'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize=(20,8))\n",
    "\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "sns.distplot(df['desc_len'], ax=axs[0])\n",
    "sns.distplot(df['short_desc_len'], ax=axs[1])\n",
    "sns.distplot(df['script_len'], ax=axs[2])\n",
    "\n",
    "plt.suptitle(\"Distribution of the length of the various text\", size=16)\n",
    "axs[0].set_title(\"Long description\")\n",
    "axs[0].set_xlabel(\"Length of description\")\n",
    "\n",
    "axs[1].set_title(\"Short description\")\n",
    "axs[1].set_xlabel(\"Length of description\")\n",
    "\n",
    "axs[2].set_title(\"Script\")\n",
    "axs[2].set_xlabel(\"Length of description\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_final_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_final_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling using NMF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df['desc_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, NMF\n",
    "from wordcloud import WordCloud\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total amount of words in vocabulary:\", len(vectorizer.vocabulary_))\n",
    "print(\"Number of documents: \", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(194, 78%%, %d%%)\" % random.randint(30, 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 6\n",
    "nmf = NMF(n_components=n_components, init='random')\n",
    "W = nmf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = []\n",
    "for i in range(max(inv_vocab.keys())+1):\n",
    "    feature_names.append(inv_vocab[i])\n",
    "n_top_words = 10\n",
    "SIZE = 100000\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(f\"Topic {topic_idx+1} - top words: \", top)\n",
    "    freqDict = {}\n",
    "    for i in range(max(inv_vocab.keys())):\n",
    "        freqDict[feature_names[i]] = int(topic[i]*SIZE)\n",
    "    wc = WordCloud(width=1600, height=800, background_color=\"rgba(255, 255, 255, 0)\", mode=\"RGBA\")\n",
    "    wc.generate_from_frequencies(freqDict)\n",
    "\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.imshow(wc.recolor(color_func=color_func, random_state=3), interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to represent the uniqueness of each episode and make the *textual data* more descriptive, *TF-IDF analysis* was considered. However, David M. Blei argues that LDA addresses the shortcomings of the TF-IDF analysis and leaves that approach behind [[REF]](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). Thus, we decided against using TF-IDF analysis for our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Descriptive Stats <a class=\"anchor\" id=\"four\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final data set is comprised of 600 rows where each row represents an episode of *The Simpsons*, and 13 feature columns. Each episode gets an overall rating between 0.0 and 10.0, although the minimum and maximum ratings in this dataset are $3.9$ and $9.3$ respectively. \n",
    "\n",
    "Each episode (row) has a feature called `desc` which holds the words used to describe the episode thoroughly, and a column called `short_desc` which represent a shortened description found on each episode's wiki page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tables show some basic stats about the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='season',y='rating',data=df.groupby('season').mean()['rating'].reset_index())\n",
    "plt.title(\"Average rating per season\", size=16)\n",
    "plt.xlabel(\"Season\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='no_overall',y='rating',data=df)\n",
    "plt.title(\"Ratings of episodes\", size=16)\n",
    "plt.xlabel(\"No. of Episode\")\n",
    "plt.ylabel(\"Rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(df[['rating','no_in_season','season']].pivot_table(columns='season',index='no_in_season',values='rating'),\n",
    "                 annot=True,\n",
    "                 cmap='coolwarm',\n",
    "                linewidth=0.05,\n",
    "                linecolor='black')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Heatmap showing the ratings for episodes\", size=16)\n",
    "plt.ylabel(\"Number of episode in season\")\n",
    "plt.xlabel(\"Season\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The past three plots visualize the decline over the years. While each season has its lowpoints, the quality (rating) of episodes has been going down over the past years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(df.corr(), annot=True,vmin=0.0, vmax=1.0, cmap='coolwarm',linewidths=1) #notation: \"annot\" not \"annote\"\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Heatplot showing the correlation between variables\", size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some correlation between how many are watching the episode and how highly it is rated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('rainbow')\n",
    "sns.distplot(df['rating'], rug=True)\n",
    "plt.title(\"Distribution of ratings of episodes\", size=16)\n",
    "plt.xlabel(\"Rating (0-10)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most episodes are scoring a rating between 6 and 8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['mil_viewers'], rug=True)\n",
    "plt.title(\"Distribution of viewers for the episodes\", size=16)\n",
    "plt.xlabel(\"Viewers (millions)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most episodes have around 5-15 million viewers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(df[df['desc_len']>0]['desc_len'], rug=True)\n",
    "plt.title(\"The Distribution of the Lenght of the Long Description of Episodes (non-unique words)\", size=16)\n",
    "plt.xlabel(\"Total Words in Description\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[df['short_desc_len']>0]['short_desc_len'], rug=True)\n",
    "plt.title(\"The Distribution of the Lenght of the Short Description of Episodes (non-unique words)\", size=16)\n",
    "plt.xlabel(\"Total Words in Description\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Linear Regression - For comparison <a class=\"anchor\" id=\"reg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see if the new topical model is more accurate than a normal linear regression model, a simple linear regression model was created in order to try and predict the rating of an episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['no_overall','no_in_season','season','mil_viewers']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "X = (X - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = y.mean()\n",
    "y_std = y.std()\n",
    "y = (y - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,predictions)\n",
    "plt.title(\"Actual values vs. Predicted values of ratings of episodes\", size=16)\n",
    "plt.xlabel(\"Test value (Standardized)\")\n",
    "plt.ylabel(\"Predicted value (Standardized)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot((y_test-predictions),bins=50);\n",
    "plt.title(\"Distribution of residuals\", size=16)\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this it can be seen that creating a linear regression model in order to predict an episode's rating from other features is somewhat achievable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generative Story & PGM <a class=\"anchor\" id=\"six\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will deploy LDA for our topic model, the generative story for the inference of topics will follow the same generative story LDA follows. Here, we have changed the wording to fit our project. The following will be the generative story our model will follow: \n",
    "\n",
    "1. For each topic $k \\in \\{1,2,...,K\\}$ where $K$ is the assumed total number of topics, there is a vector of $C$ words, $\\phi_k$, such that $p(\\theta_k|\\beta) = \\text{Dir}(\\beta)$\n",
    "\n",
    "2. For each document (*episode*) $i$:\n",
    "\n",
    ">a) There is a vector of $K$ topics, $\\theta_i$, such that $p(\\theta_i|\\alpha) = \\text{Dir}(\\alpha)$ \n",
    "    \n",
    ">b) For each of the words $j$ in each episode $i$, where $j\\in \\{1,2,...,||w_i||\\}$, we have\n",
    "    \n",
    ">>i) A **topic assignment** $z_{i,j} \\in \\{1,2,...,K\\}$ such that $p(z_{i,j}|\\theta_i) = \\text{Cat}(\\theta_i)$\n",
    "    \n",
    ">>ii) A **word** $w_{i,j}$ such that $p(w_{i,j}|\\phi_{z_{i,j}}) = \\text{Cat}(\\phi_{z_{i,j}})$\n",
    "\n",
    "What this means is that each episode will get assigned topic proportions and each topic will get assigned word proportions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First version of our PGM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](lda_pgm_mkk.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_columns = ['topic '+str(k) for k in range(1,33)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.read_csv('topic_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first version of our PGM is shown here above. We plan to build on top of it, create a dynamic topic model with inspo from [here](https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Time Dynamic Topic Model\n",
    "\n",
    "\n",
    ">\"LDA is a very powerful technique for the qualitative analysis of large corpora because of its highly interpretable topics, and it is also useful for dimensionality reduction as it transforms sparse document-term matrices into fixed low dimensional document-topic matrices. However, LDA ignores the temporal aspect present in many document collections.\" - [source](https://towardsdatascience.com/exploring-the-un-general-debates-with-dynamic-topic-models-72dc0e307696)\n",
    "\n",
    "with great insporation from [here](https://arxiv.org/pdf/1206.3298.pdf) and [here](https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pgm_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $i, j (j>i>0)$, be two arbitarty time $indexes, s_i$ and $s_j$ be the time stamps, and $\\Delta_{s_j,s_i}$ be the elapsed time between them.\n",
    "\n",
    "1) For each topic $k$, 1$\\leq$$k$$\\leq$$K$,\n",
    "\n",
    "* Draw $\\beta_{0,k}$ $\\sim$ $\\mathcal{N}$( $m,v_{0}I$)\n",
    "\n",
    "\n",
    "2) For document $d_t$ at time $s_t$ (t>0),\n",
    "* For each topic $k$, 1$\\leq k \\leq K$,\n",
    "    * From the Brownian motion model, draw \n",
    "    $\\beta_{t,k}|\\beta_{t-1,k}, s \\sim \\mathcal{N} (\\beta_{t-1,k},v\\Delta _{s_{t}}I)$\n",
    "* Draw $\\theta_{t} \\sim$ Mult($\\theta_{t}$) \n",
    "* For each word\n",
    "    * Draw $z_{t,n} \\sim $ Mult($\\theta_t$)\n",
    "    * Draw $w_{t,n} \\sim $ Mult($\\pi(\\beta_{t,z_{t,n}}$) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evolution of the topic parameters $\\beta_t$ is governed by Brownian motion. \n",
    "\n",
    "The variable $s_t$ is the observed time stamp of document $d_t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Stan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model, first version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_STAN=\"\"\"\n",
    "data{\n",
    "    int<lower=1> I; \n",
    "    int<lower=1> J[I];  // We mean J=||W_i||\n",
    "    int<lower=2> K; // # of topics\n",
    "    int<lower=2> C; \n",
    "    vector<lower=0>[K] alpha;\n",
    "    vector<lower=0>[C] beta;\n",
    "    int<lower=2> MAX_J;\n",
    "    int W[I,MAX_J];\n",
    "}\n",
    "\n",
    "parameters{\n",
    "    simplex[K] theta[I];\n",
    "    simplex[C] phi[K];\n",
    "}\n",
    "\n",
    "model{\n",
    "    for (k in 1:K)\n",
    "        phi[k]~dirichlet(beta);\n",
    "        \n",
    "    for (i in 1:I){\n",
    "        theta[i]~dirichlet(alpha);\n",
    "        \n",
    "        for (j in 1:J[i]){\n",
    "                real gamma[K];\n",
    "                for (k in 1:K)\n",
    "                    // log(P(z = k | theta)) + log(P(W | phi, z = k)) \n",
    "                    gamma[k]=log(theta[i,k])+log(phi[k,W[i][j]]);\n",
    "                \n",
    "                target+=(log_sum_exp(gamma)); //likelihood\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I: number of documents (episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = len(df)\n",
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J = ||w_i|| : number of words in each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = df['desc_len'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C: number of words in our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = set(desc_all_words)\n",
    "dictionary = list(dictionary)\n",
    "C = len(dictionary)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.desc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for rownumber in range(len(df.desc)):\n",
    "#    df.desc[rownumber] = re.sub(r'[^\\w]', ' ', df.desc[rownumber])\n",
    "#    df.desc[rownumber] = df.desc[rownumber].split()\n",
    "    \n",
    "numbered_dic = {ni: indi for indi, ni in enumerate(dictionary)}\n",
    "\n",
    "w = np.zeros((I, np.max(J)), dtype=int )\n",
    "for i in range(I): # I = number of episodes\n",
    "    for j in range(J[i]): # J = number of words per episode\n",
    "        w[i,j] = numbered_dic.get(df.desc[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.ones(K)\n",
    "beta = np.ones(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('I:',I)\n",
    "print('J;',J)\n",
    "print('K:',K)\n",
    "print('C:',C)\n",
    "print('alpha:',alpha)\n",
    "print('beta',beta)\n",
    "print('MAX_J',np.max(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42IESSJFkhH_",
    "outputId": "f277c285-17bd-4f06-e990-b0b6974dcf4b"
   },
   "outputs": [],
   "source": [
    "## Compile and collect data\n",
    "sm = pystan.StanModel(model_code=LDA_STAN)\n",
    "data={'I':I, 'J':J, 'K':K, 'C':C, 'alpha':alpha, 'beta':beta, 'MAX_J':np.max(J), 'W':w+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ha7wPzrkhIB",
    "outputId": "e7d5f799-1475-4594-9fe3-f8a643002232"
   },
   "outputs": [],
   "source": [
    "## Sample with VB\n",
    "fit = sm.vb(data=data, iter=10000, algorithm=\"meanfield\", elbo_samples=100, grad_samples=20, seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MxkMdWukhID"
   },
   "source": [
    "Extract results from STAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tIeNRMy5khID"
   },
   "outputs": [],
   "source": [
    "theta_hat = pystan_utils.vb_extract_variable(fit, \"theta\", var_type=\"matrix\", dims=[I,K])\n",
    "phi_hat = pystan_utils.vb_extract_variable(fit, \"phi\", var_type=\"matrix\", dims=[K,C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHzDl0tskhIO",
    "outputId": "33fc4cee-9d37-4891-fe63-6ce44b8d8aad",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.argmax(phi_hat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(theta_hat))\n",
    "print(np.shape(phi_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(phi_max)\n",
    "phi_max[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): #I:\n",
    "    print(\"\\n\\nEpisode: \", i)\n",
    "    print(\"max topic: \", np.argmax(theta_hat[i], axis=0))\n",
    "    print('top 20 words:')\n",
    "    for k in range(K):\n",
    "        top20 = phi_hat[k].argsort()[-20:][::-1] # get the index of the 20 largest values\n",
    "        for topword in top20:\n",
    "            for word, number in numbered_dic.items():\n",
    "                if number == topword:\n",
    "                    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(phi_hat, columns = numbered_dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the topic proportions of each episode to our df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['topic '+str(k) for k in range(1,K+1) ]\n",
    "\n",
    "theta_df = pd.DataFrame(theta_hat, columns = columns)\n",
    "\n",
    "df_topic = df.join(theta_df)\n",
    "df_topic['topic 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic.to_csv('topic_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sama og hér fyrir ofan, nema topics = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.ones(K)\n",
    "beta = np.ones(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42IESSJFkhH_",
    "outputId": "f277c285-17bd-4f06-e990-b0b6974dcf4b"
   },
   "outputs": [],
   "source": [
    "## Compile and collect data\n",
    "sm = pystan.StanModel(model_code=LDA_STAN)\n",
    "data={'I':I, 'J':J, 'K':K, 'C':C, 'alpha':alpha, 'beta':beta, 'MAX_J':np.max(J), 'W':w+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ha7wPzrkhIB",
    "outputId": "e7d5f799-1475-4594-9fe3-f8a643002232"
   },
   "outputs": [],
   "source": [
    "## Sample with VB\n",
    "fit = sm.vb(data=data, iter=10000, algorithm=\"meanfield\", elbo_samples=100, grad_samples=20, seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tIeNRMy5khID"
   },
   "outputs": [],
   "source": [
    "theta_hat32 = pystan_utils.vb_extract_variable(fit, \"theta\", var_type=\"matrix\", dims=[I,K])\n",
    "phi_hat32 = pystan_utils.vb_extract_variable(fit, \"phi\", var_type=\"matrix\", dims=[K,C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
