{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "<span style=\"float: right; color: rgb(128, 128, 128); font-size:150%\" >  <strong> Final Project - Milestone 1</span>\n",
    "<span style=\"float: left; color: rgb(128, 128, 128); font-size:150%\"> <strong>  42186 Model-based machine learning (F20)</span>    \n",
    "    <br>  \n",
    "  \n",
    "    \n",
    "    \n",
    "  <span style=\"float: right; color: rgb(128, 128, 128)\" >  <strong> Students:</span>\n",
    "  <span style=\"float: left; color: rgb(128, 128, 128)\"> <strong>  Professors:</span>\n",
    "<div> \n",
    "      <br>  \n",
    "<div>\n",
    "  <span style=\"float: right\" > Guðlaug Erlendsdóttir, s185717</span> \n",
    "  <span style=\"float: left\"> Filipe Rodrigues</span>\n",
    "<div> \n",
    "    <br>\n",
    "  <span style=\"float: right\" > Matthías Karl Karlsson, s182306</span> \n",
    "  <span style=\"float: left\"> Francisco Camara Pereira</span>\n",
    "    <br>\n",
    "<div style=\"text-align: right\"> Steinn Orri Erlendsson, s153716</div>\n",
    "    \n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "### [1. Description of Project](#one)\n",
    "\n",
    "* ####  [1.1. Research Question](#ResearchQuestion)\n",
    "\n",
    "### [2. Data](#two)\n",
    "\n",
    "* #### [2.1 Data Collection and Preprocessing](#two.one)\n",
    "\n",
    "### [3.Natural Language Preprocessing](#three)\n",
    "\n",
    "### [4. Descriptive Stats](#four)\n",
    "\n",
    "### [5. Linear Regression - For comparison](#reg)\n",
    "\n",
    "### [6. Generative Story & PGM](#six)\n",
    "\n",
    "### [7. Inital STAN Model](#eight)\n",
    "\n",
    "### [ Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simpsons - Topic Modeling with Latent Dirichlet Allocation(LDA)\n",
    "\n",
    "\n",
    "In this notebook, topic modeling through Latent Dirichlet Allocation(LDA) is used in order to try uncover the topics that have come up on episodes of The Simpsons through the years. \n",
    "\n",
    "Attributes such as `rating` and `viewers` are extracted from [IMDB](IMDB) for the first 600 episodes of The Simpsons. \n",
    "Then, for each of these episodes, the description is scraped from [The Simpsons Fandom](page) page. \n",
    "There is a lot of data cleaning to be done, as there are a lot of missing values that resulted from the web scraping due to some inconstistency in the pages' setup. \n",
    "Natural Language Processing is deployed in order to represent the textual data better. Each episode's description was put through *TF-IDF analysis* in order to obtain a better representation of their description. \n",
    "\n",
    "By using LDA for topic modeling, the episodes can be seen as a mixture of topics, and therefore a relationhip between what is happening in the episode and its rating could potentially be made. Finally, a simple linear regression is used for comparison of the model's performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Description of Project<a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "For those that have ever watched the television show *The Simpsons*, it is quite known that the quality of episodes have dwindled over the past years. The show has been running for 30 years and it has long gone past its glory days. But what is the reason behind this decline in quality? In this notebook, the topics of each episode of *The Simpsons* is investigated and through *topic modeling* episodes are clustered together in order to see whether the topics of episodes have anything to do with their respective ratings. \n",
    "\n",
    "Ratings for 600 episodes were collected from [IMDB](https://www.imdb.com/) and descriptions for each of those episodes was collected from [The Simpsons Wiki Fanpage](https://simpsons.fandom.com/wiki/List_of_Episodes). This information, along with additional information collected from [Wikipedia](https://www.wikipedia.org) such as viewers per episode and airdate, was used in order to create a data set.\n",
    "\n",
    "The probabilistic topic model *Latent Dirichlet Allocation* was used in order to extract topic proportions and allocations for each episode. These topics were then investigated in order to see if there is any apparent relationship between the topic of episodes over the years and their declining ratings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Research Question<a class=\"anchor\" id=\"ResearchQuestion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using *Topic Modeling* and *Latent Dirichlet Allocation*, we hope to uncover some of the hidden features of the episodes. In general, an episode's rating is related to what is happening in the episode. We want to find if there a relationship between the topic of episodes and their respective ratings, so our research question is;\n",
    "\n",
    "> How do the topic proportions align with the episodes and can they be used to determine an episode's rating\n",
    "\n",
    "If the topic allocations and proportions of an episode can help determine an episode's rating, not only have be created a better linear regression model, but we have proven a hypothesis which does seem logical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data<a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "In this section, the method of collecting and preprocessing both the *ratings* and *description* for each episode is shown. All of the data is collected through web scraping. The websites that were scraped were [IMDB](https://www.imdb.com), [Wikipedia](https://www.wikipedia.org), and [The Simpsons Fandom Wiki Page](https://simpsons.fandom.com/wiki/List_of_Episodes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "import os.path\n",
    "import nltk, re, pprint\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import collections\n",
    "import colorsys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.display import Image\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "import pystan\n",
    "import pystan_utils\n",
    "#make margins smaller \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "for char in ['episode','begins']:\n",
    "    stop_words.append(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Color Palettes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAABKCAYAAABtjfnGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAADHUlEQVR4nO3awWojdRzA8V/MShKVYRV1s6SQ3g2eAl18Bt+gUOjRt+grSPMCLeQFPFY8+wDexFNLoBJrdVjYpTAdD+phSRpYyI9Jhs/nOH8Cvx8hfDOTdOq6rgMAEn3Q9AAAtJ/YAJBObABIJzYApHu26bAsyyjL8p1rDw8PcXNzE4eHh9HtdlOHA2A/VFUVy+UyJpNJ9Pv9lfONsbm4uIjZbJY2HADtMp/PYzqdrlzvbPrr87o7m8ViEScnJ9F59V10Bs+3P+kO+GY6anqEVN9+NWx6hFRffzFoeoQ03d9+bnqEVHc//dj0CKl+/eGXpkdIcx+P8X28jaurqxiPxyvnG+9siqKIoijWnnUGz6Pz0WfbmXLHDD79sukRUn3+4mXTI6Qavfy46RHSdP9u52fufx9+svr4pU3uOi3+mfy/25anfl5p8eYA7AqxASCd2ACQTmwASCc2AKQTGwDSiQ0A6cQGgHRiA0A6sQEgndgAkE5sAEgnNgCkExsA0okNAOnEBoB0YgNAOrEBIJ3YAJBObABIJzYApBMbANKJDQDpxAaAdGIDQDqxASCd2ACQTmwASCc2AKQTGwDSiQ0A6cQGgHRiA0A6sQEgndgAkE5sAEgnNgCkExsA0okNAOnEBoB0YgNAOrEBIJ3YAJBObABIJzYApBMbANKJDQDpxAaAdGIDQDqxASCd2ACQ7tmmw7IsoyzLd64tFouIiKjf/JU3VcPe3A+aHiHVH7+3+zvG4rG97193+WfTI6S6e/226RFS3dWPTY+Q5j7+3a2qqrXnnbqu66defH5+HrPZLGcyAFpnPp/HdDpdub4xNuvubK6vr+P09DQuLy9jNBptf9KG3d7exvHxcczn8xgOh02Ps3X2219t3i3CfvuuqqpYLpcxmUyi3++vnG98jFYURRRFsfZsNBrFwcHBdqbcQcPh0H57rM37tXm3CPvts/F4/ORZux/eA7ATxAaAdGIDQLru2dnZ2fu+qNfrxdHRUfR6vYSRmme//dbm/dq8W4T92mzjv9EAYBs8RgMgndgAkE5sAEgnNgCkExsA0v0DqSKvI83oWxEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAABKCAYAAABtjfnGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAADI0lEQVR4nO3awWpjZRjH4bd2JMHOHNCRTiSF1IXMJstCr8CrKBS6du2+FxEvIIXcRL2G2Q0qLluCnUlUPDIybckcF+JiSCYg5uVrDs+zPB+F/7coP057dpqmaQIAEn1UegAA7Sc2AKQTGwDSiQ0A6R6tO6zrOuq6fu/Z3d1dXF9fx+HhYezu7qaOA2A7LBaLmM1mMRwOo9vtLp2vjc14PI7RaJQ2DoB2mUwmcXR0tPR8Z92nz6vebKbTaZyensbw6++is7e/+aUPwPPnn5aekOqrg/vSE1J9+fiX0hPSfP76h9ITUjU/vyw9IdX8xU+lJ6SZ397Htz9O4/LyMgaDwdL52jebqqqiqqqVZ529/eg+/mIzKx+YJ589LT0h1dP9dsfm2ZN3pSekefauvSGNiGhe7ZWekGq3+3HpCek+9O8VHwgAkE5sAEgnNgCkExsA0okNAOnEBoB0YgNAOrEBIJ3YAJBObABIJzYApBMbANKJDQDpxAaAdGIDQDqxASCd2ACQTmwASCc2AKQTGwDSiQ0A6cQGgHRiA0A6sQEgndgAkE5sAEgnNgCkExsA0okNAOnEBoB0YgNAOrEBIJ3YAJBObABIJzYApBMbANKJDQDpxAaAdGIDQDqxASCd2ACQTmwASCc2AKQTGwDSiQ0A6cQGgHRiA0A6sQEgndgAkE5sAEgnNgCke7TusK7rqOv6vWfT6TQiIm7fvM5bVdifv70tPSHVr5/cl56QqvrrVekJaRbz30tPSNX88ab0hFTzt+393Zvf/nO3xWKx8nxtbMbjcYxGo5VnL7//5n9Oe7helB4AsKVms1kMBoOl5ztN0zQf+qFVbzZXV1dxdnYWFxcX0e/3N7+0sJubmzg5OYnJZBK9Xq/0nI1zv+3V5rtFuN+2WywWMZvNYjgcRrfbXTpf+2ZTVVVUVbXyrN/vx8HBwWZWPkC9Xs/9tlib79fmu0W43zZb9UbzLx8IAJBObABIJzYApNs9Pz8//68/1Ol04vj4ODqdTsKk8txvu7X5fm2+W4T7tdnar9EAYBP8GQ2AdGIDQDqxASCd2ACQTmwASPc3kHm4aqByF8YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAABKCAYAAABtjfnGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAADHElEQVR4nO3asWpbZxjH4dd2iJQmPdBmiIgM8l5tFXjtXOgVGAym0KFX4Xuw5oINuoNOpmvnbu1uo2BbbSGnKYmdnKhzkCwo6OWzDs8zng/B/8PGPyRraz6fzwMAEm2XHgBA+4kNAOnEBoB0YgNAukerDuu6jrquP3l2d3cXl5eXsbe3Fzs7O6njANgMTdPEbDaL4XAY3W534XxlbE5PT2M8HqeNA6BdJpNJjEajhedbq776vOydzXQ6jcPDw/hp/9t48eTp+pc+AL8Pfi49IdWvX5ZekOuXx9+VnpBm+8NXpSek+ubvr0tPSPXDb+9KT0hzfXsT3//xY5yfn8dgMFg4X/nOpqqqqKpq6dmLJ0+j/9nn61n5wNxU7f5X1rMvSi/Itd1p5+9lRMTO++elJ6R61rwsPSHVy87b0hPS3ffvlXb/VQXgQRAbANKJDQDpxAaAdGIDQDqxASCd2ACQTmwASCc2AKQTGwDSiQ0A6cQGgHRiA0A6sQEgndgAkE5sAEgnNgCkExsA0okNAOnEBoB0YgNAOrEBIJ3YAJBObABIJzYApBMbANKJDQDpxAaAdGIDQDqxASCd2ACQTmwASCc2AKQTGwDSiQ0A6cQGgHRiA0A6sQEgndgAkE5sAEgnNgCkExsA0okNAOnEBoB0YgNAOrEBIJ3YAJBObABIJzYApBMbANKJDQDpHq06rOs66rr+5Nl0Oo2IiOu3/+atKuzP+mPpCanerPypb76Pj/8pPSHPh79KL0j15vWr0hNSvbp9V3pCmuvbm4iIaJpm6fnWfD6f3/fik5OTGI/HOcsAaJ3JZBKj0Wjh+crYLHtnc3FxEUdHR3F2dhb9fn/9Swu7urqKg4ODmEwm0ev1Ss9ZO/fbXG2+W4T7bbqmaWI2m8VwOIxut7twvvIDlaqqoqqqpWf9fj92d3fXs/IB6vV67rfB2ny/Nt8twv022WAwuPfMFwQASCc2AKQTGwDS7RwfHx//3xd1Op3Y39+PTqeTMKk899tsbb5fm+8W4X5ttvLbaACwDj5GAyCd2ACQTmwASCc2AKQTGwDS/QcthLLU1nYPZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAABKCAYAAABtjfnGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAADHElEQVR4nO3aQWobdxTA4afYjpTUmUXThYgM0gG0NPgwBoMPUvAdogvIoHX3vko3RUbFMKSEaUhaJ+PJKosgRSVEj7Gm37ecvwXvMcI/xp5e0zRNAECiJ20PAED3iQ0A6cQGgHRiA0C6w22HVVVFVVVfXbu/v4/b29uYTCZxcHCQOhwA+6Gu6yjLMqbTaQwGg7XzrbGZz+cxm83ShgOgWxaLRZyenq5d72179XnTk81qtYqLi4t4+moSvcOj3U/6CNQvR22PkKr+pdv7PXT4/o2Pf2p7hFST4+dtj5Bq/KK79+/d2zfx2+tf4+bmJsbj8dr51ieboiiiKIqNZ73Do3hy9HQ3Uz4yD4Nuf+F7zzff08548XPbE6Q5Ko7bHiHVs47vV3R8v4j45r9XvCAAQDqxASCd2ACQTmwASCc2AKQTGwDSiQ0A6cQGgHRiA0A6sQEgndgAkE5sAEgnNgCkExsA0okNAOnEBoB0YgNAOrEBIJ3YAJBObABIJzYApBMbANKJDQDpxAaAdGIDQDqxASCd2ACQTmwASCc2AKQTGwDSiQ0A6cQGgHRiA0A6sQEgndgAkE5sAEgnNgCkExsA0okNAOnEBoB0YgNAOrEBIJ3YAJBObABIJzYApBMbANKJDQDpxAaAdGIDQDqxASCd2ACQ7nDbYVVVUVXVV9dWq1VERDSfPsZD3lytav553/YIqZr31X//0D77+6+2J0jzsfm37RFSfag/tD1CqupTd3+3vHv7JiIi6rreeL41NvP5PGaz2caz+z//+LHJHrPl721PwA/otT1AomXbAyTr+n7/B2VZxng8Xrvea5qm+daHNj3ZLJfLuLy8jOvr6xiNRruftGV3d3dxfn4ei8UihsNh2+PsnP32V5d3i7DfvqvrOsqyjOl0GoPBYO1865NNURRRFMXGs9FoFCcnJ7uZ8hEaDof222Nd3q/Lu0XYb59teqL5wgsCAKQTGwDSiQ0A6Q6urq6uvvdD/X4/zs7Oot/vJ4zUPvvtty7v1+XdIuzXZVvfRgOAXfBnNADSiQ0A6cQGgHRiA0A6sQEg3WcwrLIS3fwkqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAABKCAYAAABtjfnGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAADFklEQVR4nO3bz2qjVQDG4ZPpQILi559xNJhCegFZBuYWvAYrlV6JdSsuKs0NtJCtrtz0DuYKXLhrCUSiM+MnytAhxoWroW1AyMtpwvMsv0PhPYvyI+nXzmq1WhUACHpUewAAu09sAIgTGwDixAaAuMfrDtu2LW3bvvXs5uamXF9fl4ODg7K3txcdB8B2WC6XZbFYlNFoVHq93q3ztbE5Pz8vk8kkNg6A3TKdTst4PL71vLPu1ee7PtnMZrNydHRUjr8+Lc2Tp5tf+gB8+9PPtSdEffPFL7UnRH3//IfaE2J+fPNV7QlRi9Oz2hOiPj39vPaEmPnL1+XL756Xy8vLMhwOb52v/WTTNE1pmubusydPy4ef9Dez8qF599faC6I+6r9Xe0LUow86tSfEDG7erz0ham/H/+3vs4/fqT0h7r4/r3hBAIA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiBMbAOLEBoA4sQEgTmwAiHu87rBt29K27VvPZrPZf2e/L3KravvrRe0FUS/mf9aeEPXPq1XtCTGzN3/UnhC16HRqT4ha/fZ37Qkx85evSymlLJfLO887q9Xq3t/Ms7OzMplMMssA2DnT6bSMx+Nbz9fG5q5PNldXV+X4+LhcXFyUwWCw+aWVzefzcnh4WKbTaen3+7XnbJz7ba9dvlsp7rftlstlWSwWZTQalV6vd+t87ddoTdOUpmnuPBsMBmV/f38zKx+gfr/vfltsl++3y3crxf222XA4vPfMCwIAxIkNAHFiA0Dc3snJycn//aFut1uePXtWut1uYFJ97rfddvl+u3y3Utxvl619Gw0ANsHXaADEiQ0AcWIDQJzYABAnNgDE/QvZe7bohBog6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.palplot(sns.color_palette(\"RdBu_r\", 7))\n",
    "sns.palplot(sns.color_palette(\"coolwarm\", 7))\n",
    "sns.palplot(sns.color_palette(\"husl\", 7))\n",
    "sns.palplot(sns.color_palette(\"PuBuGn_d\", 7))\n",
    "sns.palplot(sns.color_palette(\"Paired\", 7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Collection and Preprocessing<a class=\"anchor\" id=\"two.one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Information <a class=\"anchor\" id=\"two.one.one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all episode names, dates and relevant information is possible by scraping Wikipedia. This will serve as a basis for the episode information which will be used throughout the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_1_20_url = 'https://en.wikipedia.org/wiki/List_of_The_Simpsons_episodes_(seasons_1%E2%80%9320)#Episodes'\n",
    "episode_21_31_url = \"https://en.wikipedia.org/wiki/List_of_The_Simpsons_episodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(episode_1_20_url).text\n",
    "soup = BeautifulSoup(res,'html')\n",
    "arr = []\n",
    "next_season = 1\n",
    "for table in soup.find_all('table', class_='wikiepisodetable'):\n",
    "    if(table.previous.previous.previous.text != 'The Simpsons Movie'):\n",
    "        season = table.previous.previous.previous.text.split('(')[1].split(')')[0].split(\" \")[1]\n",
    "        for row in table.find_all('tr')[1::1]:\n",
    "            data = row.find_all(['th','td'])\n",
    "            no = data[0].text\n",
    "            no_in_season = data[1].text\n",
    "            episode_name = data[2].text[1:len(data[2].text)-1] #Remove the \" \" \n",
    "            airdate = data[5].text\n",
    "            viewers = data[7].text.split(\"[\")[0]\n",
    "            arr.append([no,season,no_in_season,episode_name,airdate,viewers])\n",
    "df_1 = pd.DataFrame(arr)\n",
    "df_1.columns = ['no_overall','season','no_in_season','episode_name','airdate','viewers (millions)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasons 1 to 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>December 17, 1989 (1989-12-17)</td>\n",
       "      <td>26.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart the Genius</td>\n",
       "      <td>January 14, 1990 (1990-01-14)</td>\n",
       "      <td>24.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer's Odyssey</td>\n",
       "      <td>January 21, 1990 (1990-01-21)</td>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  no_overall season no_in_season                       episode_name  \\\n",
       "0          1      1            1  Simpsons Roasting on an Open Fire   \n",
       "1          2      1            2                    Bart the Genius   \n",
       "2          3      1            3                    Homer's Odyssey   \n",
       "\n",
       "                          airdate viewers (millions)  \n",
       "0  December 17, 1989 (1989-12-17)               26.7  \n",
       "1   January 14, 1990 (1990-01-14)               24.5  \n",
       "2   January 21, 1990 (1990-01-21)               27.5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>439</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>Waverly Hills, 9-0-2-1-D'oh</td>\n",
       "      <td>May 3, 2009 (2009-05-03)</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>440</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Four Great Women and a Manicure</td>\n",
       "      <td>May 10, 2009 (2009-05-10)</td>\n",
       "      <td>5.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>441</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>Coming to Homerica</td>\n",
       "      <td>May 17, 2009 (2009-05-17)</td>\n",
       "      <td>5.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_overall season no_in_season                     episode_name  \\\n",
       "438        439     20           19      Waverly Hills, 9-0-2-1-D'oh   \n",
       "439        440     20           20  Four Great Women and a Manicure   \n",
       "440        441     20           21               Coming to Homerica   \n",
       "\n",
       "                       airdate viewers (millions)  \n",
       "438   May 3, 2009 (2009-05-03)               6.75  \n",
       "439  May 10, 2009 (2009-05-10)               5.16  \n",
       "440  May 17, 2009 (2009-05-17)               5.86  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the rest of the seasons are retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(episode_21_31_url).text\n",
    "soup = BeautifulSoup(res,'html')\n",
    "arr = []\n",
    "next_season = 1\n",
    "for table in soup.find_all('table', class_='wikiepisodetable'):\n",
    "    if(table.previous.previous.previous.text != 'The Longest Daycare' and table.previous.previous.previous.text):\n",
    "        season = table.previous.previous.previous.text.split('(')[1].split(')')[0].split(\" \")[1]\n",
    "        for row in table.find_all('tr')[1::1]:\n",
    "            data = row.find_all(['th','td'])\n",
    "            no = data[0].text\n",
    "            no_in_season = data[1].text\n",
    "            episode_name = data[2].text[1:len(data[2].text)-1] #Remove the \" \" \n",
    "            airdate = data[5].text\n",
    "            viewers = data[7].text.split(\"[\")[0]\n",
    "            arr.append([no,season,no_in_season,episode_name,airdate,viewers])\n",
    "        if(season=='30'):\n",
    "            break\n",
    "df_2 = pd.DataFrame(arr)\n",
    "df_2.columns = ['no_overall','season','no_in_season','episode_name','airdate','viewers (millions)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasons 21 to 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>442</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer the Whopper</td>\n",
       "      <td>September 27, 2009 (2009-09-27)</td>\n",
       "      <td>8.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>443</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart Gets a 'Z'</td>\n",
       "      <td>October 4, 2009 (2009-10-04)</td>\n",
       "      <td>9.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>444</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>The Great Wife Hope</td>\n",
       "      <td>October 11, 2009 (2009-10-11)</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  no_overall season no_in_season         episode_name  \\\n",
       "0        442     21            1    Homer the Whopper   \n",
       "1        443     21            2      Bart Gets a 'Z'   \n",
       "2        444     21            3  The Great Wife Hope   \n",
       "\n",
       "                           airdate viewers (millions)  \n",
       "0  September 27, 2009 (2009-09-27)               8.31  \n",
       "1     October 4, 2009 (2009-10-04)               9.32  \n",
       "2    October 11, 2009 (2009-10-11)                7.5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>660</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>D'oh Canada</td>\n",
       "      <td>April 28, 2019 (2019-04-28)</td>\n",
       "      <td>1.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>661</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>Woo-Hoo Dunnit?</td>\n",
       "      <td>May 5, 2019 (2019-05-05)</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>662</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>Crystal Blue-Haired Persuasion</td>\n",
       "      <td>May 12, 2019 (2019-05-12)</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    no_overall season no_in_season                    episode_name  \\\n",
       "217        660     30           21                     D'oh Canada   \n",
       "218        661     30           22                 Woo-Hoo Dunnit?   \n",
       "219        662     30           23  Crystal Blue-Haired Persuasion   \n",
       "\n",
       "                         airdate viewers (millions)  \n",
       "217  April 28, 2019 (2019-04-28)               1.93  \n",
       "218     May 5, 2019 (2019-05-05)               1.79  \n",
       "219    May 12, 2019 (2019-05-12)               1.50  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinating them together will create the final dataframe for the episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes = pd.concat([df_1,df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes.reset_index(inplace=True)\n",
    "df_episodes.drop('index',axis=1,inplace=True)\n",
    "\n",
    "df_episodes['no_overall'] = df_episodes['no_overall'].apply(lambda x: int(x))\n",
    "df_episodes['season'] = df_episodes['season'].apply(lambda x: int(x))\n",
    "df_episodes['no_in_season'] = df_episodes['no_in_season'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to clean **season 28**, here episode **607**, are two seperate episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_overall                                   608609\n",
       "season                                           28\n",
       "no_in_season                                   1213\n",
       "episode_name          The Great Phatsby Parts 1 & 2\n",
       "airdate               January 15, 2017 (2017-01-15)\n",
       "viewers (millions)                             6.90\n",
       "Name: 607, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes.iloc[607]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the episode up into episode **608** and **609** will make the data make more sense, as the overall episode count will be linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes.drop(607,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into = pd.DataFrame(data = [{'no_overall':608, 'season':28,'no_in_season': 12, \n",
    "                            'episode_name':'The Great Phatsby','airdate': 'January 15, 2017 (2017-01-15)', \n",
    "                            'viewers (millions)': 6.90}, {'no_overall':609, 'season':28,'no_in_season': 13, \n",
    "                            'episode_name':'The Great Phatsby','airdate': 'January 15, 2017 (2017-01-15)', \n",
    "                            'viewers (millions)': 6.90}], index=[607,608])\n",
    "\n",
    "df_episodes = pd.concat([df_episodes.iloc[:608], insert_into, df_episodes[608:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Ratings <a class=\"anchor\" id=\"two.one.two\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the ratings for each episode is possible by scraping IMDB. There exist several pages of ratings for the series on IMDB. Going through each one of them will yield every episode's rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_ratings_url = 'https://www.imdb.com/list/ls052175596/?st_dt=&mode=simple&page={}&ref_=ttls_vw_smp&sort=release_date,asc'\n",
    "pages = range(1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = []\n",
    "for page in range(1,8):\n",
    "    url = imdb_ratings_url.format(page)\n",
    "    res = requests.get(url).text\n",
    "    soup = BeautifulSoup(res,'html')\n",
    "    container = soup.find('div',class_='lister')\n",
    "    for item in container.find_all('div', class_='lister-item'):\n",
    "        as_ = item.find('span',class_='lister-item-header').find_all('a')\n",
    "        episode_name = as_[1].text\n",
    "        rating = item.find('div',class_='col-imdb-rating').text.strip()\n",
    "        ratings.append([episode_name,rating])\n",
    "\n",
    "df_ratings = pd.DataFrame(ratings)\n",
    "df_ratings.columns = ['episode_name','IMDB rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>Todd, Todd, Why Hast Thou Forsaken Me?</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>Bobby, It's Cold Outside</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>Hail to the Teeth</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>The Miseducation of Lisa Simpson</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Frinkcoin</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>Bart the Bad Guy</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>Screenless</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>Better Off Ned</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>Highway to Well</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>The Incredible Lightness of Being a Baby</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 episode_name IMDB rating\n",
       "669    Todd, Todd, Why Hast Thou Forsaken Me?         6.4\n",
       "670                  Bobby, It's Cold Outside         6.7\n",
       "671                         Hail to the Teeth           6\n",
       "672          The Miseducation of Lisa Simpson         6.6\n",
       "673                                 Frinkcoin         5.9\n",
       "674                          Bart the Bad Guy         7.3\n",
       "675                                Screenless         6.6\n",
       "676                            Better Off Ned         6.5\n",
       "677                           Highway to Well           7\n",
       "678  The Incredible Lightness of Being a Baby         6.4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular project, only the first 660 episodes will be investigated. This decision was made as the last episodes do not have as much description written about them as they are quite new. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = df_ratings[:661]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_name</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>Woo-Hoo Dunnit?</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>Crystal Blue-Haired Persuasion</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       episode_name IMDB rating\n",
       "659                 Woo-Hoo Dunnit?         5.9\n",
       "660  Crystal Blue-Haired Persuasion         5.8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinating these two DataFrames will yield more informative data. However, a simple merge on the names will not do because the names are not exactly identical on IMDB and Wikipedia. The `NaN` values will be looked at and manually fixed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings = pd.merge(df_episodes, df_ratings, on='episode_name',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following episodes do not get a rating after the join because the naming convention are different between IMDB and Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Bart Gets an \"F\"</td>\n",
       "      <td>October 11, 1990 (1990-10-11)</td>\n",
       "      <td>33.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>Itchy &amp; Scratchy &amp; Marge</td>\n",
       "      <td>December 20, 1990 (1990-12-20)</td>\n",
       "      <td>22.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>Bart's Dog Gets an \"F\"</td>\n",
       "      <td>March 7, 1991 (1991-03-07)</td>\n",
       "      <td>23.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>Boy-Scoutz 'n the Hood</td>\n",
       "      <td>November 18, 1993 (1993-11-18)</td>\n",
       "      <td>20.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>$pringfield (or, How I Learned to Stop Worryin...</td>\n",
       "      <td>December 16, 1993 (1993-12-16)</td>\n",
       "      <td>17.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>124</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>The PTA Disbands!</td>\n",
       "      <td>April 16, 1995 (1995-04-16)</td>\n",
       "      <td>11.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>Who Shot Mr. Burns? (Part One)</td>\n",
       "      <td>May 21, 1995 (1995-05-21)</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>129</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Who Shot Mr. Burns? (Part Two)</td>\n",
       "      <td>September 17, 1995 (1995-09-17)</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>131</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>Home Sweet Homediddly-Dum-Doodily</td>\n",
       "      <td>October 1, 1995 (1995-10-01)</td>\n",
       "      <td>14.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>135</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>King-Size Homer</td>\n",
       "      <td>November 5, 1995 (1995-11-05)</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>138</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>The Simpsons 138th Episode Spectacular</td>\n",
       "      <td>December 3, 1995 (1995-12-03)</td>\n",
       "      <td>16.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>Raging Abe Simpson and His Grumbling Grandson ...</td>\n",
       "      <td>April 28, 1996 (1996-04-28)</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>153</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>Summer of 4 Ft. 2</td>\n",
       "      <td>May 19, 1996 (1996-05-19)</td>\n",
       "      <td>14.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>162</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>El Viaje Misterioso de Nuestro Jomer (The Myst...</td>\n",
       "      <td>January 5, 1997 (1997-01-05)</td>\n",
       "      <td>14.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>171</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>Homer vs. the Eighteenth Amendment</td>\n",
       "      <td>March 16, 1997 (1997-03-16)</td>\n",
       "      <td>14.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>210</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>Lisa Gets an \"A\"</td>\n",
       "      <td>November 22, 1998 (1998-11-22)</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>211</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>Homer Simpson in: \"Kidney Trouble\"</td>\n",
       "      <td>December 6, 1998 (1998-12-06)</td>\n",
       "      <td>7.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>218</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>Marge Simpson in: \"Screaming Yellow Honkers\"</td>\n",
       "      <td>February 21, 1999 (1999-02-21)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>223</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>The Old Man and the \"C\" Student</td>\n",
       "      <td>April 25, 1999 (1999-04-25)</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>226</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>Thirty Minutes over Tokyo</td>\n",
       "      <td>May 16, 1999 (1999-05-16)</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>240</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>Alone Again, Natura-Diddily</td>\n",
       "      <td>February 13, 2000 (2000-02-13)</td>\n",
       "      <td>10.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>252</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>Lisa the Tree Hugger</td>\n",
       "      <td>November 19, 2000 (2000-11-19)</td>\n",
       "      <td>14.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>257</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>HOMR</td>\n",
       "      <td>January 7, 2001 (2001-01-07)</td>\n",
       "      <td>18.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>263</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>Hungry, Hungry Homer</td>\n",
       "      <td>March 4, 2001 (2001-03-04)</td>\n",
       "      <td>17.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>264</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>Bye Bye Nerdie</td>\n",
       "      <td>March 11, 2001 (2001-03-11)</td>\n",
       "      <td>16.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>287</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>I Am Furious (Yellow)</td>\n",
       "      <td>April 28, 2002 (2002-04-28)</td>\n",
       "      <td>12.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>291</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>Poppa's Got a Brand New Badge</td>\n",
       "      <td>May 22, 2002 (2002-05-22)</td>\n",
       "      <td>8.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>294</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>Bart vs. Lisa vs. the Third Grade</td>\n",
       "      <td>November 17, 2002 (2002-11-17)</td>\n",
       "      <td>13.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>319</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>Today I Am a Clown</td>\n",
       "      <td>December 7, 2003 (2003-12-07)</td>\n",
       "      <td>10.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>322</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>I, (Annoyed Grunt)-bot</td>\n",
       "      <td>January 11, 2004 (2004-01-11)</td>\n",
       "      <td>16.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>328</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Co-Dependents' Day</td>\n",
       "      <td>March 21, 2004 (2004-03-21)</td>\n",
       "      <td>11.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>354</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>Thank God, It's Doomsday</td>\n",
       "      <td>May 8, 2005 (2005-05-08)</td>\n",
       "      <td>10.05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>378</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>Marge and Homer Turn a Couple Play</td>\n",
       "      <td>May 21, 2006 (2006-05-21)</td>\n",
       "      <td>8.23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>380</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>Jazzy and the Pussycats</td>\n",
       "      <td>September 17, 2006 (2006-09-17)</td>\n",
       "      <td>8.94</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>385</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>Ice Cream of Margie (with the Light Blue Hair)</td>\n",
       "      <td>November 26, 2006 (2006-11-26)</td>\n",
       "      <td>10.90</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>387</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>Kill Gil, Volumes I &amp; II</td>\n",
       "      <td>December 17, 2006 (2006-12-17)</td>\n",
       "      <td>8.96</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>393</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>Rome-Old and Juli-Eh</td>\n",
       "      <td>March 11, 2007 (2007-03-11)</td>\n",
       "      <td>8.98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>Stop! Or My Dog Will Shoot</td>\n",
       "      <td>May 13, 2007 (2007-05-13)</td>\n",
       "      <td>6.48</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>402</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>The Homer of Seville</td>\n",
       "      <td>September 30, 2007 (2007-09-30)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>413</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>The Debarted</td>\n",
       "      <td>March 2, 2008 (2008-03-02)</td>\n",
       "      <td>8.18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>414</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>Dial \"N\" for Nerder</td>\n",
       "      <td>March 9, 2008 (2008-03-09)</td>\n",
       "      <td>7.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>439</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>Waverly Hills, 9-0-2-1-D'oh</td>\n",
       "      <td>May 3, 2009 (2009-05-03)</td>\n",
       "      <td>6.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>449</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>O Brother, Where Bart Thou?</td>\n",
       "      <td>December 13, 2009 (2009-12-13)</td>\n",
       "      <td>7.11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>461</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>To Surveil with Love</td>\n",
       "      <td>May 2, 2010 (2010-05-02)</td>\n",
       "      <td>6.06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>480</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>A Midsummer's Nice Dream</td>\n",
       "      <td>March 13, 2011 (2011-03-13)</td>\n",
       "      <td>5.44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>487</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>The Falcon and the D'ohman</td>\n",
       "      <td>September 25, 2011 (2011-09-25)</td>\n",
       "      <td>8.08</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>505</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>A Totally Fun Thing That Bart Will Never Do Again</td>\n",
       "      <td>April 29, 2012 (2012-04-29)</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>507</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>Ned 'n' Edna's Blend Agenda</td>\n",
       "      <td>May 13, 2012 (2012-05-13)</td>\n",
       "      <td>4.07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>519</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>The Changing of the Guardian</td>\n",
       "      <td>January 27, 2013 (2013-01-27)</td>\n",
       "      <td>5.23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>520</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>Love Is a Many-Splintered Thing</td>\n",
       "      <td>February 10, 2013 (2013-02-10)</td>\n",
       "      <td>4.19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>523</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>Black Eyed, Please</td>\n",
       "      <td>March 10, 2013 (2013-03-10)</td>\n",
       "      <td>4.85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>576</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>Cue Detective</td>\n",
       "      <td>October 4, 2015 (2015-10-04)</td>\n",
       "      <td>6.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>581</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>Lisa with an 'S'</td>\n",
       "      <td>November 22, 2015 (2015-11-22)</td>\n",
       "      <td>5.64</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>601</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>Trust but Clarify</td>\n",
       "      <td>October 23, 2016 (2016-10-23)</td>\n",
       "      <td>3.36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>624</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>The Old Blue Mayor She Ain't What She Used to Be</td>\n",
       "      <td>November 12, 2017 (2017-11-12)</td>\n",
       "      <td>4.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>630</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>Homer Is Where the Art Isn't</td>\n",
       "      <td>March 18, 2018 (2018-03-18)</td>\n",
       "      <td>2.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall  season  no_in_season  \\\n",
       "13           14       2             1   \n",
       "21           22       2             9   \n",
       "28           29       2            16   \n",
       "88           89       5             8   \n",
       "90           91       5            10   \n",
       "123         124       6            21   \n",
       "127         128       6            25   \n",
       "128         129       7             1   \n",
       "130         131       7             3   \n",
       "134         135       7             7   \n",
       "137         138       7            10   \n",
       "149         150       7            22   \n",
       "152         153       7            25   \n",
       "161         162       8             9   \n",
       "170         171       8            18   \n",
       "209         210      10             7   \n",
       "210         211      10             8   \n",
       "217         218      10            15   \n",
       "222         223      10            20   \n",
       "225         226      10            23   \n",
       "239         240      11            14   \n",
       "251         252      12             4   \n",
       "256         257      12             9   \n",
       "262         263      12            15   \n",
       "263         264      12            16   \n",
       "286         287      13            18   \n",
       "290         291      13            22   \n",
       "293         294      14             3   \n",
       "318         319      15             6   \n",
       "321         322      15             9   \n",
       "327         328      15            15   \n",
       "353         354      16            19   \n",
       "377         378      17            22   \n",
       "379         380      18             2   \n",
       "384         385      18             7   \n",
       "386         387      18             9   \n",
       "392         393      18            15   \n",
       "397         398      18            20   \n",
       "401         402      19             2   \n",
       "412         413      19            13   \n",
       "413         414      19            14   \n",
       "438         439      20            19   \n",
       "448         449      21             8   \n",
       "460         461      21            20   \n",
       "479         480      22            16   \n",
       "486         487      23             1   \n",
       "504         505      23            19   \n",
       "506         507      23            21   \n",
       "518         519      24            11   \n",
       "519         520      24            12   \n",
       "522         523      24            15   \n",
       "575         576      27             2   \n",
       "580         581      27             7   \n",
       "600         601      28             5   \n",
       "623         624      29             6   \n",
       "629         630      29            12   \n",
       "\n",
       "                                          episode_name  \\\n",
       "13                                    Bart Gets an \"F\"   \n",
       "21                            Itchy & Scratchy & Marge   \n",
       "28                              Bart's Dog Gets an \"F\"   \n",
       "88                              Boy-Scoutz 'n the Hood   \n",
       "90   $pringfield (or, How I Learned to Stop Worryin...   \n",
       "123                                  The PTA Disbands!   \n",
       "127                     Who Shot Mr. Burns? (Part One)   \n",
       "128                     Who Shot Mr. Burns? (Part Two)   \n",
       "130                  Home Sweet Homediddly-Dum-Doodily   \n",
       "134                                    King-Size Homer   \n",
       "137             The Simpsons 138th Episode Spectacular   \n",
       "149  Raging Abe Simpson and His Grumbling Grandson ...   \n",
       "152                                  Summer of 4 Ft. 2   \n",
       "161  El Viaje Misterioso de Nuestro Jomer (The Myst...   \n",
       "170                 Homer vs. the Eighteenth Amendment   \n",
       "209                                   Lisa Gets an \"A\"   \n",
       "210                 Homer Simpson in: \"Kidney Trouble\"   \n",
       "217       Marge Simpson in: \"Screaming Yellow Honkers\"   \n",
       "222                    The Old Man and the \"C\" Student   \n",
       "225                          Thirty Minutes over Tokyo   \n",
       "239                        Alone Again, Natura-Diddily   \n",
       "251                               Lisa the Tree Hugger   \n",
       "256                                               HOMR   \n",
       "262                               Hungry, Hungry Homer   \n",
       "263                                     Bye Bye Nerdie   \n",
       "286                              I Am Furious (Yellow)   \n",
       "290                      Poppa's Got a Brand New Badge   \n",
       "293                  Bart vs. Lisa vs. the Third Grade   \n",
       "318                                 Today I Am a Clown   \n",
       "321                             I, (Annoyed Grunt)-bot   \n",
       "327                                 Co-Dependents' Day   \n",
       "353                           Thank God, It's Doomsday   \n",
       "377                 Marge and Homer Turn a Couple Play   \n",
       "379                            Jazzy and the Pussycats   \n",
       "384     Ice Cream of Margie (with the Light Blue Hair)   \n",
       "386                           Kill Gil, Volumes I & II   \n",
       "392                               Rome-Old and Juli-Eh   \n",
       "397                         Stop! Or My Dog Will Shoot   \n",
       "401                               The Homer of Seville   \n",
       "412                                       The Debarted   \n",
       "413                                Dial \"N\" for Nerder   \n",
       "438                        Waverly Hills, 9-0-2-1-D'oh   \n",
       "448                        O Brother, Where Bart Thou?   \n",
       "460                               To Surveil with Love   \n",
       "479                           A Midsummer's Nice Dream   \n",
       "486                         The Falcon and the D'ohman   \n",
       "504  A Totally Fun Thing That Bart Will Never Do Again   \n",
       "506                        Ned 'n' Edna's Blend Agenda   \n",
       "518                       The Changing of the Guardian   \n",
       "519                    Love Is a Many-Splintered Thing   \n",
       "522                                 Black Eyed, Please   \n",
       "575                                      Cue Detective   \n",
       "580                                   Lisa with an 'S'   \n",
       "600                                  Trust but Clarify   \n",
       "623   The Old Blue Mayor She Ain't What She Used to Be   \n",
       "629                       Homer Is Where the Art Isn't   \n",
       "\n",
       "                             airdate viewers (millions) IMDB rating  \n",
       "13     October 11, 1990 (1990-10-11)               33.6         NaN  \n",
       "21    December 20, 1990 (1990-12-20)               22.2         NaN  \n",
       "28        March 7, 1991 (1991-03-07)               23.9         NaN  \n",
       "88    November 18, 1993 (1993-11-18)               20.1         NaN  \n",
       "90    December 16, 1993 (1993-12-16)               17.9         NaN  \n",
       "123      April 16, 1995 (1995-04-16)               11.8         NaN  \n",
       "127        May 21, 1995 (1995-05-21)               15.0         NaN  \n",
       "128  September 17, 1995 (1995-09-17)               16.0         NaN  \n",
       "130     October 1, 1995 (1995-10-01)               14.5         NaN  \n",
       "134    November 5, 1995 (1995-11-05)               17.0         NaN  \n",
       "137    December 3, 1995 (1995-12-03)               16.4         NaN  \n",
       "149      April 28, 1996 (1996-04-28)               13.0         NaN  \n",
       "152        May 19, 1996 (1996-05-19)               14.7         NaN  \n",
       "161     January 5, 1997 (1997-01-05)               14.9         NaN  \n",
       "170      March 16, 1997 (1997-03-16)               14.6         NaN  \n",
       "209   November 22, 1998 (1998-11-22)                  8         NaN  \n",
       "210    December 6, 1998 (1998-12-06)                7.2         NaN  \n",
       "217   February 21, 1999 (1999-02-21)                8.6         NaN  \n",
       "222      April 25, 1999 (1999-04-25)                6.9         NaN  \n",
       "225        May 16, 1999 (1999-05-16)                  8         NaN  \n",
       "239   February 13, 2000 (2000-02-13)               10.8         NaN  \n",
       "251   November 19, 2000 (2000-11-19)               14.9         NaN  \n",
       "256     January 7, 2001 (2001-01-07)               18.5         NaN  \n",
       "262       March 4, 2001 (2001-03-04)               17.6         NaN  \n",
       "263      March 11, 2001 (2001-03-11)               16.1         NaN  \n",
       "286      April 28, 2002 (2002-04-28)               12.4         NaN  \n",
       "290        May 22, 2002 (2002-05-22)                8.2         NaN  \n",
       "293   November 17, 2002 (2002-11-17)               13.3         NaN  \n",
       "318    December 7, 2003 (2003-12-07)               10.5         NaN  \n",
       "321    January 11, 2004 (2004-01-11)               16.3         NaN  \n",
       "327      March 21, 2004 (2004-03-21)               11.2         NaN  \n",
       "353         May 8, 2005 (2005-05-08)              10.05         NaN  \n",
       "377        May 21, 2006 (2006-05-21)               8.23         NaN  \n",
       "379  September 17, 2006 (2006-09-17)               8.94         NaN  \n",
       "384   November 26, 2006 (2006-11-26)              10.90         NaN  \n",
       "386   December 17, 2006 (2006-12-17)               8.96         NaN  \n",
       "392      March 11, 2007 (2007-03-11)               8.98         NaN  \n",
       "397        May 13, 2007 (2007-05-13)               6.48         NaN  \n",
       "401  September 30, 2007 (2007-09-30)                8.4         NaN  \n",
       "412       March 2, 2008 (2008-03-02)               8.18         NaN  \n",
       "413       March 9, 2008 (2008-03-09)                7.3         NaN  \n",
       "438         May 3, 2009 (2009-05-03)               6.75         NaN  \n",
       "448   December 13, 2009 (2009-12-13)               7.11         NaN  \n",
       "460         May 2, 2010 (2010-05-02)               6.06         NaN  \n",
       "479      March 13, 2011 (2011-03-13)               5.44         NaN  \n",
       "486  September 25, 2011 (2011-09-25)               8.08         NaN  \n",
       "504      April 29, 2012 (2012-04-29)               5.00         NaN  \n",
       "506        May 13, 2012 (2012-05-13)               4.07         NaN  \n",
       "518    January 27, 2013 (2013-01-27)               5.23         NaN  \n",
       "519   February 10, 2013 (2013-02-10)               4.19         NaN  \n",
       "522      March 10, 2013 (2013-03-10)               4.85         NaN  \n",
       "575     October 4, 2015 (2015-10-04)               6.02         NaN  \n",
       "580   November 22, 2015 (2015-11-22)               5.64         NaN  \n",
       "600    October 23, 2016 (2016-10-23)               3.36         NaN  \n",
       "623   November 12, 2017 (2017-11-12)               4.75         NaN  \n",
       "629      March 18, 2018 (2018-03-18)               2.10         NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings[df_episodes_ratings['IMDB rating'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of the names of the episodes that are missing their ratings can be created: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bart Gets an F 8.2\n",
      "Itchy and Scratchy and Marge 8.1\n",
      "Bart's Dog Gets an F 7.5\n",
      "Boy Scoutz 'n the Hood 8.7\n",
      "$pringfield (Or, How I Learned to Stop Worrying and Love Legalized Gambling) 8.6\n",
      "The PTA Disbands 8.1\n",
      "Who Shot Mr. Burns? Part One 9.2\n",
      "Who Shot Mr. Burns? Part Two 9\n",
      "Home Sweet Home-Dum-Diddly Doodily 8.4\n",
      "King Size Homer 9\n",
      "The Simpsons 138th Episode Spectacular! 7.5\n",
      "Raging Abe Simpson and His Grumbling Grandson in 'The Curse of the Flying Hellfish' 8.4\n",
      "Summer of 4'2\" 8.4\n",
      "El Viaje Misterioso De Nuestro Jomer 8.6\n",
      "Homer vs. the 18th Amendment 8.9\n",
      "Lisa Gets an 'A' 8.1\n",
      "Homer Simpson in: 'Kidney Trouble' 7.3\n",
      "Marge Simpson in 'Screaming Yellow Honkers' 7.3\n",
      "The Old Man and the 'C' Student 7.3\n",
      "Thirty Minutes Over Tokyo 8\n",
      "Alone Again, Natura-Diddly 7.7\n",
      "Lisa the Treehugger 7.1\n",
      "Homr 8.1\n",
      "Hungry Hungry Homer 7.5\n",
      "Bye Bye Nerdy 6.6\n",
      "I Am Furious Yellow 7.7\n",
      "Papa's Got a Brand New Badge 7.8\n",
      "Bart vs. Lisa vs. 3rd Grade 7.1\n",
      "Today, I Am a Clown 6.4\n",
      "I, (Annoyed Grunt)-Bot 7.2\n",
      "Co-Dependent's Day 6.7\n",
      "Thank God It's Doomsday 7.2\n",
      "Homer and Marge Turn a Couple Play 6.3\n",
      "Jazzy & The Pussycats 6.6\n",
      "Ice Cream of Margie: With the Light Blue Hair 6.9\n",
      "Kill Gil, Vol. 1 & 2 6.2\n",
      "Rome-old and Juli-eh 6.1\n",
      "Stop or My Dog Will Shoot 6.7\n",
      "Homer of Seville 6.5\n",
      "The DeBarted 7.6\n",
      "Dial 'N' for Nerder 7.3\n",
      "Waverly Hills, 9021-D'Oh 7\n",
      "Oh Brother, Where Bart Thou? 7.1\n",
      "To Surveil, with Love 7\n",
      "A Midsummer's Nice Dreams 5.9\n",
      "The Falcon and the D'Ohman 7\n",
      "A Totally Fun Thing Bart Will Never Do Again 7.5\n",
      "Ned 'N' Edna's Blend 6.5\n",
      "Changing of the Guardian 6.3\n",
      "Love Is a Many Splintered Thing 6.1\n",
      "Black-Eyed, Please 6.8\n",
      "'Cue Detective 6.6\n",
      "Lisa with an \"S\" 5.9\n",
      "Trust But Clarify 6.3\n",
      "Singin' in the Lane 6.3\n",
      "3 Scenes Plus a Tag from a Marriage 6.6\n"
     ]
    }
   ],
   "source": [
    "missing = df_episodes_ratings[df_episodes_ratings['IMDB rating'].isna()].reset_index()\n",
    "missing_episodes = missing['index']\n",
    "for episode in missing_episodes:\n",
    "    print(df_ratings.iloc[episode]['episode_name'],df_ratings.iloc[episode]['IMDB rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen by comparing the two lists, these are the episodes that are missing a rating in the final dataframe. Now, an array can be created with the ratings of these episodes, extracted from `df_ratings`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ratings = []\n",
    "for episode in missing_episodes:\n",
    "    missing_ratings.append([episode,df_episodes.iloc[episode]['episode_name'],df_ratings.iloc[episode]['IMDB rating']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in missing_ratings: \n",
    "    index = episode[0]\n",
    "    rating = episode[2]\n",
    "    df_episodes_ratings.at[index, 'IMDB rating'] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [no_overall, season, no_in_season, episode_name, airdate, viewers (millions), IMDB rating]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings[df_episodes_ratings['IMDB rating'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame holding each episode's information along with it's rating is now complete, but the airdate column can use a little bit of cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>December 17, 1989 (1989-12-17)</td>\n",
       "      <td>26.7</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart the Genius</td>\n",
       "      <td>January 14, 1990 (1990-01-14)</td>\n",
       "      <td>24.5</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_overall  season  no_in_season                       episode_name  \\\n",
       "0           1       1             1  Simpsons Roasting on an Open Fire   \n",
       "1           2       1             2                    Bart the Genius   \n",
       "\n",
       "                          airdate viewers (millions) IMDB rating  \n",
       "0  December 17, 1989 (1989-12-17)               26.7         8.2  \n",
       "1   January 14, 1990 (1990-01-14)               24.5         7.7  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings['year'] = df_episodes_ratings['airdate'].apply(lambda x: str(x).split(\"(\")[1].split(\"-\")[0])\n",
    "df_episodes_ratings['month'] = df_episodes_ratings['airdate'].apply(lambda x: str(x).split(\"(\")[1].split(\"-\")[1].split('-')[0])\n",
    "df_episodes_ratings['day'] = df_episodes_ratings['airdate'].apply(lambda x: str(x).split(\"(\")[1].split(\"-\")[2].replace(\")\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>airdate</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>December 17, 1989 (1989-12-17)</td>\n",
       "      <td>26.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1989</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart the Genius</td>\n",
       "      <td>January 14, 1990 (1990-01-14)</td>\n",
       "      <td>24.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1990</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_overall  season  no_in_season                       episode_name  \\\n",
       "0           1       1             1  Simpsons Roasting on an Open Fire   \n",
       "1           2       1             2                    Bart the Genius   \n",
       "\n",
       "                          airdate viewers (millions) IMDB rating  year month  \\\n",
       "0  December 17, 1989 (1989-12-17)               26.7         8.2  1989    12   \n",
       "1   January 14, 1990 (1990-01-14)               24.5         7.7  1990    01   \n",
       "\n",
       "  day  \n",
       "0  17  \n",
       "1  14  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Airdate column can now be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings.drop('airdate',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the IMDB rating column should hold float values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings['IMDB rating'] = df_episodes_ratings['IMDB rating'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some `NaN` values in the column `viewers (millions)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>160</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>Lisa's Date with Density</td>\n",
       "      <td>N/A</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1996</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>161</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Hurricane Neddy</td>\n",
       "      <td>N/A</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1996</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>173</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>The Canine Mutiny</td>\n",
       "      <td>N/A</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1997</td>\n",
       "      <td>04</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall  season  no_in_season              episode_name  \\\n",
       "159         160       8             7  Lisa's Date with Density   \n",
       "160         161       8             8           Hurricane Neddy   \n",
       "172         173       8            20         The Canine Mutiny   \n",
       "\n",
       "    viewers (millions)  IMDB rating  year month day  \n",
       "159                N/A          7.8  1996    12  15  \n",
       "160                N/A          8.8  1996    12  29  \n",
       "172                N/A          7.6  1997    04  13  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings[df_episodes_ratings['viewers (millions)'] == 'N/A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values will be set as the average of the viewers for the season that they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_season_8 = np.average([float(viewer) for viewer in df_episodes_ratings[(df_episodes_ratings['viewers (millions)'] != 'N/A') & (df_episodes_ratings['season'] == 8)]['viewers (millions)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values that need to be filled in are in places 159, 160 and 172:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na_indices = [index for index in df_episodes_ratings[df_episodes_ratings['viewers (millions)'] == 'N/A'].reset_index()['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in fill_na_indices:    \n",
    "    df_episodes_ratings.at[index,'viewers (millions)'] = avg_season_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column should also hold float values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings['viewers (millions)'] = df_episodes_ratings['viewers (millions)'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_overall            0\n",
       "season                0\n",
       "no_in_season          0\n",
       "episode_name          0\n",
       "viewers (millions)    0\n",
       "IMDB rating           0\n",
       "year                  0\n",
       "month                 0\n",
       "day                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings.to_csv('episode_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpsons.fandom Pages of Each Episode <a class=\"anchor\" id=\"two.one.four\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scour through each episode's description, scraping a Wiki fan-page of the series is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_url = 'https://simpsons.fandom.com/wiki/List_of_Episodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_array = []\n",
    "res = requests.get(wiki_url).text\n",
    "soup = BeautifulSoup(res,'html')\n",
    "no_overall = 1\n",
    "for table in soup.find_all('table', class_='wikitable')[1:31]: #Skip the overview table and only take the first 30 seasons\n",
    "    for row in table.find_all('tr'):\n",
    "            links = row.find_all('a')\n",
    "            for link in links:\n",
    "                if(link['href'].split('/wiki/')[1] != 'The_Simpsons_Movie'):\n",
    "                    links_array.append([no_overall,link['href'].split('/wiki/')[1]])\n",
    "                    no_overall = no_overall + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episode_wiki = pd.DataFrame(links_array)\n",
    "df_episode_wiki.columns = ['no_overall','wiki_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>wiki_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons_Roasting_on_an_Open_Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bart_the_Genius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Homer%27s_Odyssey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>There%27s_No_Disgrace_Like_Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bart_the_General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>657</td>\n",
       "      <td>Girl%27s_in_the_Band</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>658</td>\n",
       "      <td>I%27m_Just_a_Girl_Who_Can%27t_Say_D%27oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>659</td>\n",
       "      <td>D%27oh_Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>660</td>\n",
       "      <td>Woo-hoo_Dunnit%3F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>661</td>\n",
       "      <td>Crystal_Blue-Haired_Persuasion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>661 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_overall                                 wiki_link\n",
       "0             1         Simpsons_Roasting_on_an_Open_Fire\n",
       "1             2                           Bart_the_Genius\n",
       "2             3                         Homer%27s_Odyssey\n",
       "3             4           There%27s_No_Disgrace_Like_Home\n",
       "4             5                          Bart_the_General\n",
       "..          ...                                       ...\n",
       "656         657                      Girl%27s_in_the_Band\n",
       "657         658  I%27m_Just_a_Girl_Who_Can%27t_Say_D%27oh\n",
       "658         659                             D%27oh_Canada\n",
       "659         660                         Woo-hoo_Dunnit%3F\n",
       "660         661            Crystal_Blue-Haired_Persuasion\n",
       "\n",
       "[661 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episode_wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the episode 'The Great Phatsby' (episode 608/609), is causing problems. It can be solved as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "phatsby_insert = pd.DataFrame({'no_overall':609, 'wiki_link':'The_Great_Phatsby'},index=[608])\n",
    "phatsby_insert\n",
    "df_episode_wiki = pd.concat([df_episode_wiki.iloc[:608], phatsby_insert, df_episode_wiki[608:]])\n",
    "df_episode_wiki.reset_index(inplace=True, drop=True)\n",
    "df_episode_wiki_no_correction = df_episode_wiki.iloc[609:]\n",
    "df_episode_wiki_no_correction['no_overall'] = df_episode_wiki_no_correction['no_overall'].apply(lambda x: x+1)\n",
    "df_episde_wiki = pd.concat([df_episode_wiki.iloc[:608], df_episode_wiki_no_correction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_ratings = pd.merge(df_episodes_ratings,df_episode_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_overall</th>\n",
       "      <th>season</th>\n",
       "      <th>no_in_season</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>viewers (millions)</th>\n",
       "      <th>IMDB rating</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>wiki_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Simpsons Roasting on an Open Fire</td>\n",
       "      <td>26.7</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1989</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>Simpsons_Roasting_on_an_Open_Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bart the Genius</td>\n",
       "      <td>24.5</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1990</td>\n",
       "      <td>01</td>\n",
       "      <td>14</td>\n",
       "      <td>Bart_the_Genius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer's Odyssey</td>\n",
       "      <td>27.5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1990</td>\n",
       "      <td>01</td>\n",
       "      <td>21</td>\n",
       "      <td>Homer%27s_Odyssey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>There's No Disgrace Like Home</td>\n",
       "      <td>20.2</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1990</td>\n",
       "      <td>01</td>\n",
       "      <td>28</td>\n",
       "      <td>There%27s_No_Disgrace_Like_Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Bart the General</td>\n",
       "      <td>27.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1990</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>Bart_the_General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   no_overall  season  no_in_season                       episode_name  \\\n",
       "0           1       1             1  Simpsons Roasting on an Open Fire   \n",
       "1           2       1             2                    Bart the Genius   \n",
       "2           3       1             3                    Homer's Odyssey   \n",
       "3           4       1             4      There's No Disgrace Like Home   \n",
       "4           5       1             5                   Bart the General   \n",
       "\n",
       "   viewers (millions)  IMDB rating  year month day  \\\n",
       "0                26.7          8.2  1989    12  17   \n",
       "1                24.5          7.7  1990    01  14   \n",
       "2                27.5          7.4  1990    01  21   \n",
       "3                20.2          7.7  1990    01  28   \n",
       "4                27.1          8.0  1990    02  04   \n",
       "\n",
       "                           wiki_link  \n",
       "0  Simpsons_Roasting_on_an_Open_Fire  \n",
       "1                    Bart_the_Genius  \n",
       "2                  Homer%27s_Odyssey  \n",
       "3    There%27s_No_Disgrace_Like_Home  \n",
       "4                   Bart_the_General  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes_ratings.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Text Files of Episodes <a class=\"anchor\" id=\"two.one.five\"></a>\n",
    "\n",
    "Here, each description is fetched and saved as a text file in order to attach them to the data set later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('synopsis'):\n",
    "        os.mkdir('synopsis')\n",
    "        \n",
    "not_full_story = ['Mr._Spritz_Goes_to_Washington', \n",
    "                  'Three_Gays_of_the_Condo', \n",
    "                  'The_Fat_and_the_Furriest', \n",
    "                  'Today,_I_Am_a_Clown', \n",
    "                  'Simple_Simpson', \n",
    "                  'Fat_Man_and_Little_Boy', \n",
    "                  'My_Fare_Lady', \n",
    "                  'To_Courier_with_Love', \n",
    "                  'Lisa_Gets_the_Blues', \n",
    "                  'Girl%27s_in_the_Band',\n",
    "                  'A_Star_is_Born-Again',\n",
    "                  'Milhouse_Doesn%27t_Live_Here_Anymore']\n",
    "\n",
    "for season in df_episodes_ratings['season'].unique():\n",
    "    if not os.path.exists('synopsis/'+'season'+str(season)):\n",
    "        os.mkdir('synopsis/'+'season'+str(season))\n",
    "\n",
    "save_path = os.getcwd()+'/synopsis/'\n",
    "for episode in df_episodes_ratings['wiki_link']:\n",
    "    syn = \" \"\n",
    "    \n",
    "    res = requests.get(\"https://simpsons.fandom.com/wiki/{}\".format(episode)).text\n",
    "    ep_number = df_episodes_ratings[df_episodes_ratings['wiki_link']==episode]['no_overall'].values[0]\n",
    "    season = int(df_episodes_ratings[df_episodes_ratings['wiki_link']==episode]['season'].values[0])\n",
    "    \n",
    "    soup = BeautifulSoup(res,'html')\n",
    "    \n",
    "    # Remove all images\n",
    "    figs = soup.findAll('figure')\n",
    "    for fig in figs:\n",
    "        fig.decompose()\n",
    "        \n",
    "    h3s = soup.findAll('h3')\n",
    "    for h3 in h3s:\n",
    "        h3.decompose()\n",
    "\n",
    "    if(episode in not_full_story):\n",
    "        parent = soup.find('span', {'id':'Synopsis'}).parent\n",
    "        sibling = parent.next_sibling\n",
    "        while(sibling.name == 'p' or sibling.name is None or sibling.name =='a' or sibling.name==''):\n",
    "            if(sibling.name == 'p' or sibling.name=='a'):\n",
    "                syn += ' ' + str(sibling.text)\n",
    "                sibling = sibling.next_sibling\n",
    "            else:\n",
    "                sibling = sibling.next_sibling\n",
    "\n",
    "            \n",
    "    else:\n",
    "\n",
    "        if(soup.find('span', {'id':'Full_Story'})):\n",
    "            \n",
    "            parent = soup.find('span', {'id':'Full_Story'}).parent\n",
    "            sibling = parent.next_sibling\n",
    "            while(sibling.name == 'p' or sibling.name is None or sibling.name =='a'):\n",
    "                if(sibling.name == 'p' or sibling.name=='a'):\n",
    "                    syn += ' ' + str(sibling.text)\n",
    "                    sibling = sibling.next_sibling\n",
    "                else:\n",
    "                    sibling = sibling.next_sibling\n",
    "\n",
    "        elif(soup.find('span', {'id':'The_Story'})):\n",
    "            \n",
    "            parent = soup.find('span', {'id':'The_Story'}).parent\n",
    "            sibling = parent.next_sibling\n",
    "            while(sibling.name == 'p' or sibling.name is None or sibling.name =='a'):\n",
    "                if(sibling.name == 'p' or sibling.name=='a'):\n",
    "                    syn += ' ' + str(sibling.text)\n",
    "                    sibling = sibling.next_sibling\n",
    "                else:\n",
    "                    sibling = sibling.next_sibling\n",
    "                    \n",
    "        elif(soup.find('span', {'id':'Full_story'})):\n",
    "            \n",
    "            parent = soup.find('span', {'id':'Full_story'}).parent\n",
    "            sibling = parent.next_sibling\n",
    "            while(sibling.name == 'p' or sibling.name is None or sibling.name =='a'):\n",
    "                if(sibling.name == 'p' or sibling.name=='a'):\n",
    "                    syn += ' ' + str(sibling.text)\n",
    "                    sibling = sibling.next_sibling\n",
    "                else:\n",
    "                    sibling = sibling.next_sibling\n",
    "\n",
    "        elif(soup.find('span', {'id':'Plot'})):\n",
    "            \n",
    "            parent = soup.find('span', {'id':'Plot'}).parent\n",
    "            sibling = parent.next_sibling\n",
    "            while(sibling.name == 'p' or sibling.name is None or sibling.name=='a'):\n",
    "                if(sibling.name == 'p' or sibling.name=='a'):\n",
    "                    syn += ' ' + str(sibling.text)\n",
    "                    sibling = sibling.next_sibling\n",
    "                else:\n",
    "                    sibling = sibling.next_sibling\n",
    "                    \n",
    "        elif(soup.find('span', {'id':'Full_Story.C2.A0'})):\n",
    "            \n",
    "            parent = soup.find('span', {'id':'Full_Story.C2.A0'}).parent\n",
    "            sibling = parent.next_sibling\n",
    "\n",
    "            while(sibling.name == 'p' or sibling.name is None or sibling.name =='a'):\n",
    "                if(sibling.name == 'p' or sibling.name=='a'):\n",
    "                    syn += ' ' + str(sibling.text)\n",
    "                    sibling = sibling.next_sibling\n",
    "                else:\n",
    "                    sibling = sibling.next_sibling\n",
    "\n",
    "                    \n",
    "    complete_name = os.path.join(os.path.expanduser('~'),save_path+'season'+str(season)+'/',str(ep_number)+'.txt')\n",
    "\n",
    "    with io.open(complete_name, \"w\", encoding=\"utf-8\") as f: \n",
    "        f.write(str(syn))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is then necessary to copy episode 608 and create episode 609, as these two episodes are the same episode - often referred to as episode \"608609\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.getcwd()+'/synopsis/season28/'\n",
    "path = save_path+\"608.txt\"\n",
    "text = io.open(path,'r',encoding='utf-8').read()\n",
    "with io.open(save_path+'609.txt', \"w\", encoding=\"utf-8\") as f: \n",
    "    f.write(str(text))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for ep in df_episodes_ratings['no_overall']:\n",
    "    season = df_episodes_ratings[df_episodes_ratings['no_overall']==ep]['season'].values[0]\n",
    "    with io.open(\"synopsis/season{}/{}.txt\".format(season,ep),'r', encoding='utf-8') as f:\n",
    "        desc = f.read()\n",
    "        desc = desc.lower()\n",
    "        rows.append([ep,desc])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc = pd.DataFrame(rows, columns = ['no_overall','desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc[df_desc['desc']==' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Natural Language Processing<a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "In order to represent the text data better, some useful natural language processing methods had to be used. \n",
    "\n",
    "First, all *stopwords*, along with the word \"*episode*\", will be removed from the descriptions as they do not pertain any information about the episode itself. \n",
    "\n",
    "Then, the description is split into *tokens* (words) and lastly we use stemming. The words can then be analyzed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "ps = PorterStemmer() \n",
    "\n",
    "for ep in df_episodes_ratings['no_overall']:\n",
    "    season = df_episodes_ratings[df_episodes_ratings['no_overall']==ep]['season'].values[0]\n",
    "    with io.open(\"synopsis/season{}/{}.txt\".format(season,ep),'r', encoding='utf-8') as f:\n",
    "        desc = f.read()\n",
    "        desc = re.sub(r'[^\\w]', ' ', desc)\n",
    "        desc = re.sub(\" \\d+\", \" \", desc) #removing numbers\n",
    "        desc = ps.stem(desc)\n",
    "        desc = desc.replace('  ',' ').replace('\\n','')[1:]\n",
    "        desc = [word.lower() for word in desc.split(' ') if word.lower() not in stop_words and len(word)>1]\n",
    "        rows.append([ep, desc])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc = pd.DataFrame(rows, columns = ['no_overall','desc'])\n",
    "df_desc = df_desc[:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a column is created in order to display the length of each description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc['len'] = df_desc['desc'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_episodes_ratings.merge(df_desc, on='no_overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['no_overall', 'season', 'no_in_season', 'episode_name',\n",
    "       'mil_viewers', 'rating', 'year', 'month', 'day',\n",
    "       'wiki_link', 'desc', 'len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('wiki_link',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mil_viewers'] = df['mil_viewers'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = df['day'].replace('22[a]','22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_overall'] = df['no_overall'].apply(lambda x: int(x))\n",
    "df['no_in_season'] = df['no_in_season'].apply(lambda x: int(x))\n",
    "df['season'] = df['season'].apply(lambda x: int(x))\n",
    "df['year'] = df['year'].apply(lambda x: int(x))\n",
    "df['month'] = df['month'].apply(lambda x: int(x))\n",
    "df['day'] = df['day'].apply(lambda x: int(x))\n",
    "df['rating'] = df['rating'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set now looks like this, where the column *desc* is essentially a bag of words for each episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following episodes are the episodes which do not have a description. These episodes will be removed from the dataset before any model is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['len']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for desc in df['desc']:\n",
    "    for word in desc:\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(all_words).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common words that appear in the descriptions are words that one would expect to appear often in a description of any episode of The Simpsons. In order to represent the uniqueness of each episode and make the *textual data* more descriptive, *TF-IDF analysis* was deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All English stopwords, along with the word *episode*, have been removed from the description before this point. \n",
    "\n",
    "In order to perform TF-IDF analysis, the unique set of words used over all the episodes need to be represented as a collection. \n",
    "\n",
    "The computations of the tf-idf values was greatly inspired by this following [article](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = Counter(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each episode will now have their own *term frequency*, i.e. how often a word appears in the description of the episode divided by the total words used to describe the episode.\n",
    "\n",
    "*Inverse Data Frequency* is then the log of the number of episodes, divided by the number of episodes that contain the word *w* in their description. \n",
    "\n",
    "Then, in order to compute the TF-IDF, these terms are multiplied together. Each word will get a tf-idf score for all the words that appear in all descriptions of episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "words: Array of words\n",
    "returns: Dictionary of the words with their respective tf value\n",
    "'''\n",
    "\n",
    "def tf(words):\n",
    "    from collections import Counter\n",
    "    tf_dict = {}\n",
    "    word_counts = Counter(words)\n",
    "    for word in word_counts:\n",
    "        count = word_counts[word]\n",
    "        tf_dict[word] = count / len(words)\n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    keys = []\n",
    "    for doc in documents:\n",
    "        keys += doc.keys()\n",
    "    idfDict = dict.fromkeys(keys, 0)\n",
    "    for i,document in enumerate(documents):\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(tf, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tf.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i,row in df.iterrows():\n",
    "    documents.append(Counter(row['desc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = idf(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these methods a new method can be created that returns an episode's top $200$ words relative to their tf-idf scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tf_idf(words):\n",
    "    if(len(words)<1):\n",
    "        return []\n",
    "    df_idf = pd.DataFrame.from_dict(tf_idf(tf(words),idfs),orient='index').reset_index()\n",
    "    df_idf.columns = ['word','score']\n",
    "    return list(df_idf.sort_values(by='score', ascending=False)[:200].reset_index(drop=True)['word'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column can then be added to the data set, listing the top *descriptive* words for each episode in respect to their tf-idf scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tf_idf'] = df['desc'].apply(get_top_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Descriptive Stats <a class=\"anchor\" id=\"four\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final data set is comprised of 600 rows where each row represents an episode of *The Simpsons*, and 12 feature columns. Each episode gets an overall rating between 0.0 and 10.0, although the minimum and maximum ratings in this dataset are $3.9$ and $9.3$ respectively. \n",
    "\n",
    "Each episode (row) has a feature called `desc` which holds the words used to describe the episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tables show some basic stats about the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='season',y='rating',data=df.groupby('season').mean()['rating'].reset_index())\n",
    "plt.title(\"Average rating per season\", size=16)\n",
    "plt.xlabel(\"Season\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='no_overall',y='rating',data=df)\n",
    "plt.title(\"Ratings of episodes\", size=16)\n",
    "plt.xlabel(\"No. of Episode\")\n",
    "plt.ylabel(\"Rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(df[['rating','no_in_season','season']].pivot_table(columns='season',index='no_in_season',values='rating'),\n",
    "                 annot=True,\n",
    "                 cmap='coolwarm',\n",
    "                linewidth=0.05,\n",
    "                linecolor='black')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Heatmap showing the ratings for episodes\", size=16)\n",
    "plt.ylabel(\"Number of episode in season\")\n",
    "plt.xlabel(\"Season\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The past three plots visualize the decline over the years. While each season has its lowpoints, the quality (rating) of episodes has been going down over the past years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(df.corr(), annot=True,vmin=0.0, vmax=1.0, cmap='coolwarm',linewidths=1) #notation: \"annot\" not \"annote\"\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Heatplot showing the correlation between variables\", size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some correlation between how many are watching the episode and how highly it is rated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('rainbow')\n",
    "sns.distplot(df['rating'], rug=True)\n",
    "plt.title(\"Distribution of ratings of episodes\", size=16)\n",
    "plt.xlabel(\"Rating (0-10)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most episodes are scoring a rating between 6 and 8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['mil_viewers'], rug=True)\n",
    "plt.title(\"Distribution of viewers for the episodes\", size=16)\n",
    "plt.xlabel(\"Viewers (millions)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most episodes have around 5-15 million viewers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(df[df['len']>0]['len'], rug=True)\n",
    "plt.title(\"The Distribution of the Lenght of Description of Episodes (words)\", size=16)\n",
    "plt.xlabel(\"Words in Description\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above it can be seen that most episodes have quite a large text that describes it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Linear Regression - For comparison <a class=\"anchor\" id=\"reg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see if the new topical model is more accurate than a normal linear regression model, a simple linear regression model was created in order to try and predict the rating of an episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['no_overall','no_in_season','season','mil_viewers','year','month','day']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "X = (X - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = y.mean()\n",
    "y_std = y.std()\n",
    "y = (y - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,predictions)\n",
    "plt.title(\"Actual values vs. Predicted values of ratings of episodes\", size=16)\n",
    "plt.xlabel(\"Test value (Standardized)\")\n",
    "plt.ylabel(\"Predicted value (Standardized)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot((y_test-predictions),bins=50);\n",
    "plt.title(\"Distribution of residuals\", size=16)\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this it can be seen that creating a linear regression model in order to predict an episode's rating from other features is somewhat achievable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generative Story & PGM <a class=\"anchor\" id=\"six\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will deploy LDA for our topic model, the generative story for the inference of topics will follow the same generative story LDA follows. Here, we have changed the wording to fit our project. The following will be the generative story our model will follow: \n",
    "\n",
    "1. For each topic $k \\in \\{1,2,...,K\\}$ where $K$ is the assumed total number of topics, there is a vector of $C$ words, $\\phi_k$, such that $p(\\theta_k|\\beta) = \\text{Dir}(\\beta)$\n",
    "\n",
    "2. For each document (*episode*) $i$:\n",
    "\n",
    ">a) There is a vector of $K$ topics, $\\theta_i$, such that $p(\\theta_i|\\alpha) = \\text{Dir}(\\alpha)$ \n",
    "    \n",
    ">b) For each of the words $j$ in each episode $i$, where $j\\in \\{1,2,...,||w_i||\\}$, we have\n",
    "    \n",
    ">>i) A **topic assignment** $z_{i,j} \\in \\{1,2,...,K\\}$ such that $p(z_{i,j}|\\theta_i) = \\text{Cat}(\\theta_i)$\n",
    "    \n",
    ">>ii) A **word** $w_{i,j}$ such that $p(w_{i,j}|\\phi_{z_{i,j}}) = \\text{Cat}(\\phi_{z_{i,j}})$\n",
    "\n",
    "What this means is that each episode will get assigned topic proportions and each topic will get assigned word proportions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First version of our PGM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pgm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first version of our PGM is shown here above. We plan to build on top of it, create a dynamic topic model with inspo from [here](https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Stan Model <a class=\"anchor\" id=\"eight\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is highly inspired on Notebook 8 from this course. We will later build on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with ancestral sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*Generate artificial data, and run inference on the\n",
    "model using STAN to see if it is able to recover the true values/parameters\n",
    "that were used to generate the data.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample = df.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting total number of words (after NLP) per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df.tf_idf.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I: number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = len(dfSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J = ||w_i|| : number of words in each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K: number of topics, that we will play around with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C: number of words in our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dfSample.tf_idf.explode().unique()[:30] #starting with just the first 30 words for now\n",
    "C = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros( (I, K));\n",
    "beta = 0.5*np.ones(C) ## Size C\n",
    "alpha = 0.5*np.ones(K) ## Size K\n",
    "J=[] # cutting down number of the words per episode for this sample\n",
    "\n",
    "for i in range(I):\n",
    "    theta[i] = np.random.dirichlet(alpha);\n",
    "    J.append(int(np.random.poisson(10)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define word vectors (for each topic) and normalize:\n",
    "phi = np.zeros( (K, C) );\n",
    "for k in range(K): #topics\n",
    "    phi[k] = np.random.dirichlet(beta) # ný prufa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> When we ran the above for phi, we didn't get the expected results. Would have thought we could do the same as we did with theta, but we'll have to look into how we can properply define phi. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define word vectors (for each topic) and normalize:\n",
    "phi = np.zeros( (K, C) );\n",
    "phi[0] = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "phi[0] *= 1/np.sum(phi[0])\n",
    "phi[1] = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "phi[1] *= 1/np.sum(phi[1])\n",
    "phi[2] = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
    "          0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "phi[2] *= 1/np.sum(phi[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_sample(p):\n",
    "    return list(np.random.multinomial(1, p)).index(1)\n",
    "\n",
    "def ancestral_sampling(J, theta, phi):\n",
    "    ## Initialise\n",
    "    z = np.zeros( (I, np.max(J)), dtype=int )  \n",
    "    w = np.zeros( (I, np.max(J)), dtype=int )\n",
    "\n",
    "    for i in range(I):\n",
    "        for j in range(J[i]):\n",
    "            z[i, j] = categorical_sample(theta[i])\n",
    "            w[i, j] = categorical_sample(phi[int(z[i, j])])\n",
    "    return z, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, w = ancestral_sampling(J, theta, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_STAN=\"\"\"\n",
    "data{\n",
    "    int<lower=1> I; \n",
    "    int<lower=1> J[I];  // We mean J=||W_i||\n",
    "    int<lower=2> K; // # of topics\n",
    "    int<lower=2> C; \n",
    "    vector<lower=0>[K] alpha;\n",
    "    vector<lower=0>[C] beta;\n",
    "    int<lower=2> MAX_J;\n",
    "    int W[I,MAX_J];\n",
    "}\n",
    "\n",
    "parameters{\n",
    "    simplex[K] theta[I];\n",
    "    simplex[C] phi[K];\n",
    "}\n",
    "\n",
    "model{\n",
    "    for (k in 1:K)\n",
    "        phi[k]~dirichlet(beta);\n",
    "        \n",
    "    for (i in 1:I){\n",
    "        theta[i]~dirichlet(alpha);\n",
    "        \n",
    "        for (j in 1:J[i]){\n",
    "                real gamma[K];\n",
    "                for (k in 1:K)\n",
    "                    // log(P(z = k | theta)) + log(P(W | phi, z = k)) \n",
    "                    gamma[k]=log(theta[i,k])+log(phi[k,W[i][j]]);\n",
    "                \n",
    "                target+=(log_sum_exp(gamma)); //likelihood\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42IESSJFkhH_",
    "outputId": "f277c285-17bd-4f06-e990-b0b6974dcf4b"
   },
   "outputs": [],
   "source": [
    "## Compile and collect data\n",
    "sm = pystan.StanModel(model_code=LDA_STAN)\n",
    "data={'I':I, 'J':J, 'K':K, 'C':C, 'alpha':alpha, 'beta':beta, 'MAX_J':np.max(J), 'W':w+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ha7wPzrkhIB",
    "outputId": "e7d5f799-1475-4594-9fe3-f8a643002232"
   },
   "outputs": [],
   "source": [
    "## Sample with VB\n",
    "fit = sm.vb(data=data, iter=10000, algorithm=\"meanfield\", elbo_samples=100, grad_samples=20, seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MxkMdWukhID"
   },
   "source": [
    "Extract results from STAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tIeNRMy5khID"
   },
   "outputs": [],
   "source": [
    "theta_hat = pystan_utils.vb_extract_variable(fit, \"theta\", var_type=\"matrix\", dims=[I,K])\n",
    "phi_hat = pystan_utils.vb_extract_variable(fit, \"phi\", var_type=\"matrix\", dims=[K,C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZ1RqPaKkhIL"
   },
   "source": [
    "Compare estimated $\\hat{\\phi}$ with true values of $\\phi$ used to generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wa-7T4NUkhIM",
    "outputId": "64b8990b-86ab-4f0f-a003-d36f7baf85c2"
   },
   "outputs": [],
   "source": [
    "np.argmax(phi, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHzDl0tskhIO",
    "outputId": "33fc4cee-9d37-4891-fe63-6ce44b8d8aad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argmax(phi_hat, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phi and phi_hat are matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J\n",
    "print(\"w:\", w)\n",
    "for i in range(I):\n",
    "    print(\"\\n\\nEpisode: \", i)\n",
    "    print(\"Theta: \", theta[i])\n",
    "    for j in range(J[i]):\n",
    "        print(\"Word %d: Topic assignment: %d->   %s  \" % (j,  z[i, j]+1, dictionary[w[i, j]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model, first version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we see that the model is correctly implemented and the inference also seems to be working correctly we can run it on bigger samples of our data, i.e. including all words per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample= dfSample[['rating','tf_idf', 'length']] # same sample as before,taking only the columns that we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I: number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = len(dfSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J = ||w_i|| : number of words in each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = dfSample['length'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing K, the number of topics\n",
    "\n",
    "with inspo from [here.](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#12buildingthetopicmodel).\n",
    "temporary fix in a non-STAN LDA model. planning on finding a proper way to do this, if we won't succeed we'll use trial and error to find an ok value for K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = df['tf_idf']#.to_list()\n",
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "#print(corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "import gensim\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "#pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=data_lemmatized, dictionary=id2word, coherence='c_v') #u_mass  eða c_v\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests to use K as 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C: number of words in our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dfSample.tf_idf.explode().unique() #starting with the first 30 for now\n",
    "C = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = np.zeros( (K, C) );\n",
    "for k in range(K): #topics\n",
    "    phi[k] = np.random.dirichlet(beta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_sample(p):\n",
    "    return list(np.random.multinomial(1, p)).index(1)\n",
    "\n",
    "def ancestral_sampling(J, theta, phi):\n",
    "    ## Initialise\n",
    "    z = np.zeros( (I, np.max(J)), dtype=int )  \n",
    "    w = np.zeros( (I, np.max(J)), dtype=int )\n",
    "\n",
    "    for i in range(I):\n",
    "        for j in range(J[i]):\n",
    "            z[i, j] = categorical_sample(theta[i])\n",
    "            w[i, j] = categorical_sample(phi[int(z[i, j])])\n",
    "    return z, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros( (I, K));\n",
    "beta = 0.5*np.ones(C) ## Size C\n",
    "alpha = 0.5*np.ones(K) ## Size K\n",
    "\n",
    "for i in range(I):\n",
    "    theta[i] = np.random.dirichlet(alpha);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, w = ancestral_sampling(J, theta, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42IESSJFkhH_",
    "outputId": "f277c285-17bd-4f06-e990-b0b6974dcf4b"
   },
   "outputs": [],
   "source": [
    "## Compile and collect data\n",
    "sm = pystan.StanModel(model_code=LDA_STAN)\n",
    "data={'I':I, 'J':J, 'K':K, 'C':C, 'alpha':alpha, 'beta':beta, 'MAX_J':np.max(J), 'W':w+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ha7wPzrkhIB",
    "outputId": "e7d5f799-1475-4594-9fe3-f8a643002232"
   },
   "outputs": [],
   "source": [
    "## Sample with VB\n",
    "fit = sm.vb(data=data, iter=10000, algorithm=\"meanfield\", elbo_samples=100, grad_samples=20, seed=42, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MxkMdWukhID"
   },
   "source": [
    "Extract results from STAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tIeNRMy5khID"
   },
   "outputs": [],
   "source": [
    "theta_hat = pystan_utils.vb_extract_variable(fit, \"theta\", var_type=\"matrix\", dims=[I,K])\n",
    "phi_hat = pystan_utils.vb_extract_variable(fit, \"phi\", var_type=\"matrix\", dims=[K,C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZ1RqPaKkhIL"
   },
   "source": [
    "Compare estimated $\\hat{\\phi}$ with true values of $\\phi$ used to generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wa-7T4NUkhIM",
    "outputId": "64b8990b-86ab-4f0f-a003-d36f7baf85c2"
   },
   "outputs": [],
   "source": [
    "np.argmax(phi, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHzDl0tskhIO",
    "outputId": "33fc4cee-9d37-4891-fe63-6ce44b8d8aad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argmax(phi_hat, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phi and phi_hat are matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J\n",
    "print(\"w:\", w)\n",
    "for i in range(I):\n",
    "    print(\"\\n\\nEpisode: \", i)\n",
    "    print(\"Theta: \", theta[i])\n",
    "    for j in range(J[i]):\n",
    "        print(\"Word %d: Topic assignment: %d->   %s  \" % (j,  z[i, j]+1, dictionary[w[i, j]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusion <a class=\"anchor\" id=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our research question;\n",
    "> How do the topic proportions align with the episodes and can they be used to determine an episode's rating\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
